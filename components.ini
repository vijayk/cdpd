#[component-name]
# HDP_VERSION -> Distro.ini
# git_url -> hdp-base.json
# branch -> hdp-base.json
# comp_version -> hdp-base.json
# BUILD_NUMBER -> jenkins Env
# package_count. this file -> overriden by platfom_components.txt
# build_tool -> this file
# install.cmd -> this file
# deploy.cmd  -> this file
# UnitTest Command -> this file
# depends_on -> hdp-base.json
# text-replace, xml-replace, xpath-replace - for replacing versions in component code

[accumulo]
build_tool = maven
# Maven build defaults to building with -Dhadoop.profile=3 so we do not need to explicit set that
# currently we are using maven 3.3.9 in CDPD builds and have to exclude the accumulo maven-plugin build ::due to bug: https://issues.apache.org/jira/browse/MPLUGIN-312
COMMON_BUILD_OPTS = "${MVN_CMD} -pl '!maven-plugin' -Dfindbugs.skip -Dcheckstyle.skip -Pdocs,assemble -Drepoid=${NEXUS_DEPLOY_REPO_ID} -Drepourl=${NEXUS_REPO_URL} -Dreponame=${NEXUS_DEPLOY_REPO_ID} ${MAVEN_TEST_OPTS}"
setversion_cmd = ${MVN_CMD} ${MVN_SET_VERSION_CMD} -DnewVersion=${accumulo_jar_version}
deploy_cmd = ${COMMON_BUILD_OPTS} -DskipTests deploy
coverage_tool = cobertura

    [[artifacts]]
        artifact_1 = assemble/target/accumulo-${accumulo_jar_version}-bin.tar.gz
        artifact_2 = server/native/target/accumulo-native-${accumulo_jar_version}/accumulo-native-${accumulo_jar_version}/libaccumulo.so
        artifact_3 = test.tar.gz

    [[install_cmd]]
        cmd_1 = ${COMMON_BUILD_OPTS} -DskipTests install -Dmaven.javadoc.skip=true
        cmd_2 = "tar --exclude-vcs -zcf test.tar.gz test"

    [[test_cmd]]
        cmd_1 = ${COMMON_BUILD_OPTS} clean package -Dsurefire.timeout=2400

    [[test_coverage_cmd]]
        cmd_1 = ${COMMON_BUILD_OPTS} clean cobertura:cobertura -Dsurefire.timeout=2400

    [[fortify_cmd]]
        cmd_1 = ${COMMON_BUILD_OPTS} -DskipTests install -Dmaven.javadoc.skip=true
        cmd_2 = ${COMMON_BUILD_OPTS} -DskipTests -Dmaven.javadoc.skip=true ${FORTIFY_ARGS} -Dfortify.sca.buildId=accumulo-${accumulo_jar_version} ${FORTIFY_CLEAN_CMD}
        cmd_3 = ${COMMON_BUILD_OPTS} -DskipTests -Dmaven.javadoc.skip=true ${FORTIFY_ARGS} -Dfortify.sca.buildId=accumulo-${accumulo_jar_version} -Dfortify.sca.verbose=true -Dfortify.sca.translateLogfile=translate.log -Dfortify.sca.debug=true ${FORTIFY_TRANSLATE_CMD}
        cmd_4 = sourceanalyzer -b accumulo-${accumulo_jar_version} ${FORTIFY_SCAN_LARGE_MEMORY} -scan -f accumulo-${accumulo_jar_version}.fpr


    [[xml-replace]]
        REPLACE_1 = 'hadoop.version' , ${hadoop_jar_version}, pom.xml
        REPLACE_2 = 'zookeeper.version' , ${zookeeper_jar_version}, pom.xml

# TODO (sriharsha) figure out hbase, solr version from pom.xml , BUG-58027 cached apache archive tar to s3.
# https://github.com/hortonworks/build-support/commit/6a258920f96f89aa7bec590734db69cf878c42a9
[atlas]
build_tool = maven
atlas_alternate_name = apache-atlas
hbase_pkg_version = 1.1.2
solr_pkg_version = 5.1.0
COMMON_BUILD_OPTS = "${MVN352_CMD} -Dfindbugs.skip=true -Pgpg -DskipDocs=true -Dhadoop.version=${hadoop_jar_version} -Dzookeeper.version=${zookeeper_jar_version} -Dinternal.maven.repository=${NEXUS_REPO_URL} -DStagingId=${NEXUS_DEPLOY_REPO_ID} -DStagingUrl=${NEXUS_REPO_URL} -Dhbase.tar=http://public-repo-1.hortonworks.com/ARTIFACTS/dist/hbase/${hbase_pkg_version}/hbase-${hbase_pkg_version}-bin.tar.gz -Dsolr.tar=http://public-repo-1.hortonworks.com/ARTIFACTS/dist/lucene/solr/${solr_pkg_version}/solr-${solr_pkg_version}.tgz"
setversion_cmd = ${MVN352_CMD} ${MVN_SET_VERSION_CMD} -DnewVersion=${atlas_jar_version}
deploy_cmd = ${COMMON_BUILD_OPTS} -Pdist -DskipTests -DskipITs deploy assembly:assembly
package_count = 3
coverage_tool = cobertura

    [[artifacts]]
        artifact_1 = distro/target/${atlas_alternate_name}-${atlas_jar_version}-bin.tar.gz
        artifact_2 = distro/target/${atlas_alternate_name}-${atlas_jar_version}-hbase-hook.tar.gz
        artifact_4 = distro/target/${atlas_alternate_name}-${atlas_jar_version}-sqoop-hook.tar.gz
        artifact_5 = distro/target/${atlas_alternate_name}-${atlas_jar_version}-kafka-hook.tar.gz
        artifact_6 = distro/target/${atlas_alternate_name}-${atlas_jar_version}-hive-hook.tar.gz
        artifact_7 = distro/target/${atlas_alternate_name}-${atlas_jar_version}-server.tar.gz
        artifact_8 = distro/target/${atlas_alternate_name}-${atlas_jar_version}-impala-hook.tar.gz

    [[install_cmd]]
        cmd_1 = ${COMMON_BUILD_OPTS} -Pdist -DskipTests -DskipITs install

    [[test_cmd]]
        cmd_1 = ${COMMON_BUILD_OPTS} --fail-at-end -Dsurefire.timeout=1200 -DfailIfNoTests=false test

    [[test_coverage_cmd]]
        cmd_1 = ${COMMON_BUILD_OPTS} --fail-at-end -Dsurefire.timeout=1200 -DfailIfNoTests=false cobertura:cobertura

    [[fortify_cmd]]
        cmd_1 = ${COMMON_BUILD_OPTS} -Pdist -DskipTests -DskipITs install
        cmd_2 = ${COMMON_BUILD_OPTS} -Pdist -DskipTests -DskipITs ${FORTIFY_ARGS} -Dfortify.sca.buildId=${atlas_alternate_name}-${atlas_jar_version} ${FORTIFY_CLEAN_CMD}
        cmd_3 = ${COMMON_BUILD_OPTS} -Pdist -DskipTests -DskipITs ${FORTIFY_ARGS} -Dfortify.sca.buildId=${atlas_alternate_name}-${atlas_jar_version} ${FORTIFY_TRANSLATE_CMD}
        cmd_4 = sourceanalyzer -b ${atlas_alternate_name}-${atlas_jar_version} ${FORTIFY_SCAN_MEMORY} -scan -f ${atlas_alternate_name}-${atlas_jar_version}.fpr

    [[text-replace]]
        REPLACE_1 = '0.8-incubating-SNAPSHOT', ${atlas_jar_version}, build-tools/pom.xml , regex_replace

    [[xml-replace]]
        REPLACE_1 = 'hadoop.version' , ${hadoop_jar_version} , pom.xml
        REPLACE_2 = 'hive.version' , ${hive_jar_version} , pom.xml
        REPLACE_3 = 'calcite.version', ${calcite_jar_version} , pom.xml
        REPLACE_4 = 'zookeeper.version' , ${zookeeper_jar_version} , pom.xml
        REPLACE_5 = 'hbase.version' , ${hbase_jar_version} , pom.xml
        REPLACE_6 = 'kafka.version', ${kafka_jar_version} , pom.xml


[avatica]
build_tool = maven
setversion_cmd = ${MVN_CMD} ${MVN_SET_VERSION_CMD} -DnewVersion=${avatica_jar_version}
deploy_cmd = "${MVN_CMD} deploy -Drepo.id=${NEXUS_DEPLOY_REPO_ID} -Dinternal.maven.repo=${NEXUS_REPO_URL} -DskipITs -DskipTests"
no_package = True

    [[install_cmd]]
        cmd_1 = ${MVN_CMD} install package -DskipITs -DskipTests

    [[test_cmd]]
        cmd_1 = ${MVN_CMD} test

[avro]
build_tool = maven
setversion_cmd = ${MVN_CMD} ${MVN_SET_VERSION_CMD} -DnewVersion=${avro_jar_version}
deploy_cmd = "${MVN_CMD} deploy -Drepo.id=${NEXUS_DEPLOY_REPO_ID} -Dinternal.maven.repo=${NEXUS_REPO_URL} -DskipITs -DskipTests"
no_package = True

    [[artifacts]]
        artifact_1 = avro-distro-${avro_jar_version}-lang-java.tar.gz

    [[install_cmd]]
        cmd_0 = ${MVN_CMD} -N install
        cmd_1 = ${MVN_CMD} -DskipTests -Dhadoop.version=2 -Pdist clean install javadoc:aggregate, lang/java
        cmd_2 = ${MVN_CMD} -DskipTests -Dhadoop.version=2 site, lang/java/trevni/doc
        cmd_3 = ${ANT_CMD} -Dforrest.home=${FORREST_HOME} , doc
        cmd_4 = ${MVN_CMD} -N -P copy-artifacts antrun:run
        cmd_5 = cp -f lang/java/ipc/target/avro-ipc-${avro_jar_version}-tests.jar dist/java
        cmd_6 = tar -zcvf avro-distro-${avro_jar_version}-lang-java.tar.gz dist build

    [[fortify_cmd]]
        cmd_1 = ${MVN_CMD} -DskipTests -Dhadoop.version=2 -Pdist clean install, lang/java
        cmd_2 = ${MVN_CMD} -DskipTests -Dhadoop.version=2 -Pdist ${FORTIFY_ARGS} -Dfortify.sca.buildId=avro-${avro_jar_version} ${FORTIFY_CLEAN_CMD}, lang/java
        cmd_3 = ${MVN_CMD} -DskipTests -Dhadoop.version=2 -Pdist ${FORTIFY_ARGS} -Dfortify.sca.buildId=avro-${avro_jar_version} ${FORTIFY_TRANSLATE_CMD}, lang/java
        cmd_4 = sourceanalyzer -b avro-${avro_jar_version} ${FORTIFY_SCAN_MEMORY} -scan -f avro-${avro_jar_version}.fpr

    [[test_cmd]]
        cmd_1 = ${MVN_CMD} test

    [[text-replace]]
        REPLACE_1 = ".+" , "${avro_jar_version}" , share/VERSION.txt , regex_replace

    [[xml-replace]]
        REPLACE_1 = 'hadoop2.version' , ${hadoop_jar_version} , lang/java/pom.xml

[arrow]
build_tool = maven
setversion_cmd = ${MVN_CMD} ${MVN_SET_VERSION_CMD} -DnewVersion=${arrow_jar_version}, java
deploy_cmd = "${MVN_CMD} deploy -Drepo.id=${NEXUS_DEPLOY_REPO_ID} -Dinternal.maven.repo=${NEXUS_REPO_URL} -DskipITs -DskipTests", java
no_package = True

    [[install_cmd]]
        cmd_1 = ${MVN_CMD} install, java

    [[fortify_cmd]]
        cmd_1 = ${MVN_CMD} clean install, java
        cmd_2 = ${MVN_CMD} ${FORTIFY_ARGS} -Dfortify.sca.buildId=arrow-${arrow_jar_version} ${FORTIFY_CLEAN_CMD}, java
        cmd_3 = ${MVN_CMD} ${FORTIFY_ARGS} -Dfortify.sca.buildId=arrow-${arrow_jar_version} ${FORTIFY_TRANSLATE_CMD}, java
        cmd_4 = sourceanalyzer -b arrow-${arrow_jar_version} ${FORTIFY_SCAN_MEMORY} -scan -f arrow-${arrow_jar_version}.fpr

    [[xml-replace]]
        REPLACE_1 = 'dep.hadoop.version' , ${hadoop_jar_version} , java/pom.xml

[bigtop-jsvc]
build_tool = ant
    [[install_cmd]]
        cmd_0 = mkdir -p ${TAR_DIR}/bigtop-jsvc , ${BASE_DIR}
        cmd_1 = "wget -O ${TAR_DIR}/bigtop-jsvc/commons-daemon-${JSVC_VERSION}.tar.gz http://dev.hortonworks.com.s3.amazonaws.com/ARTIFACTS/dist/commons/daemon/source/commons-daemon-${JSVC_VERSION}-native-src.tar.gz" , ${BASE_DIR}

[bigtop-utils]
build_tool = bash
    [[install_cmd]]
        cmd_0 = mkdir -p ${TAR_DIR}/bigtop-utils , ${BASE_DIR}

[calcite]
build_tool = maven
setversion_cmd = ${MVN_CMD} ${MVN_SET_VERSION_CMD} -DnewVersion=${calcite_jar_version}
deploy_cmd = "${MVN_CMD} deploy -Drepo.id=${NEXUS_DEPLOY_REPO_ID} -Dinternal.maven.repo=${NEXUS_REPO_URL} -DskipITs -DskipTests"
no_package = True
FORTIFY_VERSION=17.20
FORTIFY_SCA_HOME="${TOOLS_HOME}/fortify_sca_17.20"

    [[install_cmd]]
        cmd_1 = ${MVN_CMD} install package -DskipITs -DskipTests

    [[test_cmd]]
        cmd_1 = ${MVN_CMD} -DfailIfNoTests=false -Dmaven.test.failure.ignore=true test

    [[fortify_cmd]]
        cmd_1 = ${MVN_CMD} -DskipITs -DskipTests install
        cmd_2 = ${MVN_CMD} -DskipITs -DskipTests -Dfortify.sca.Xmx=48G -Dfortify.sca.Xss=400M -Dfortify.sca.sourceanalyzer.executable=${TOOLS_HOME}/fortify_sca_17.20/bin/sourceanalyzer -Dfortify.sca.source.version=1.8 -Dfortify.sca.cp=${MR_HOME}/repository -Dfortify.sca.buildId=calcite-${calcite_jar_version} ${FORTIFY_CLEAN_CMD}
        cmd_3 = ${MVN_CMD} -DskipITs -DskipTests -Dfortify.sca.Xmx=48G -Dfortify.sca.Xss=400M -Dfortify.sca.sourceanalyzer.executable=${TOOLS_HOME}/fortify_sca_17.20/bin/sourceanalyzer -Dfortify.sca.source.version=1.8 -Dfortify.sca.cp=${MR_HOME}/repository -Dfortify.sca.verbose=true -Dfortify.sca.translateLogfile=translate.log -Dfortify.sca.debug=true -Dfortify.sca.buildId=calcite-${calcite_jar_version} ${FORTIFY_TRANSLATE_CMD}
        cmd_4 = sourceanalyzer -b calcite-${calcite_jar_version} ${FORTIFY_SCAN_LARGE_MEMORY} -scan -f calcite-${calcite_jar_version}.fpr

    [[xml-replace]]
        REPLACE_1 = 'activeByDefault' , false , pom.xml
        REPLACE_2 = 'avatica.version', ${avatica_jar_version}, pom.xml

[crunch]
build_tool = maven
setversion_cmd = ${MVN_CMD} ${MVN_SET_VERSION_CMD} -DnewVersion=${crunch_jar_version}
JAVA_HOME=/grid/0/jenkins/tools/jdk8/jdk1.8.0_171
no_package = True
FORTIFY_VERSION=17.20
FORTIFY_SCA_HOME="${TOOLS_HOME}/fortify_sca_17.20"

    [[artifacts]]
        artifact_1 = crunch-dist/target/apache-crunch-${crunch_jar_version}-bin.zip
        artifact_2 = crunch-dist/target/apache-crunch-${crunch_jar_version}-bin.tar.gz

    [[install_cmd]]
        cmd_1 = ${MVN_CMD} -DskipTests -Dcrunch.platform=2 -Dslf4j.version=1.6.1 -Papache-release source:jar-no-fork javadoc:jar install

    [[fortify_cmd]]
        cmd_1 = ${MVN_CMD} -DskipTests -Dcrunch.platform=2 -Dslf4j.version=1.6.1 -Papache-release install
        cmd_2 = ${MVN_CMD} -DskipTests -Dcrunch.platform=2 -Dslf4j.version=1.6.1 -Papache-release -Dfortify.sca.Xmx=48G -Dfortify.sca.Xss=400M -Dfortify.sca.sourceanalyzer.executable=${TOOLS_HOME}/fortify_sca_17.20/bin/sourceanalyzer -Dfortify.sca.source.version=1.8 -Dfortify.sca.cp=${MR_HOME}/repository -Dfortify.sca.buildId=crunch-${crunch_jar_version} ${FORTIFY_CLEAN_CMD}
        cmd_3 = ${MVN_CMD} -DskipTests -Dcrunch.platform=2 -Dslf4j.version=1.6.1 -Papache-release -Dfortify.sca.Xmx=48G -Dfortify.sca.Xss=400M -Dfortify.sca.sourceanalyzer.executable=${TOOLS_HOME}/fortify_sca_17.20/bin/sourceanalyzer -Dfortify.sca.source.version=1.8 -Dfortify.sca.cp=${MR_HOME}/repository -Dfortify.sca.buildId=crunch-${crunch_jar_version} ${FORTIFY_TRANSLATE_CMD}
        cmd_4 = sourceanalyzer -b crunch-${crunch_jar_version} ${FORTIFY_SCAN_LARGE_MEMORY} -scan -f crunch-${crunch_jar_version}.fpr

    [[text-replace]]
        REPLACE_1 = '\$\{cdh.zookeeper.version\}', ${zookeeper_jar_version}, crunch-spark/pom.xml, regex_replace
        REPLACE_2 = '\$\{cdh.hbase.version\}', ${hbase_jar_version}, crunch-spark/pom.xml, regex_replace

    [[xml-replace]]
        REPLACE_1 = 'guava.version' , '11.0.2', pom.xml
        REPLACE_2 = 'commons-io.version' , '2.6', pom.xml
        REPLACE_3 = 'commons-lang.version' , '2.6', pom.xml
        REPLACE_4 = 'commons-codec.version' , '1.9', pom.xml
        REPLACE_5 = 'commons-logging.version' , '1.1.3', pom.xml
        REPLACE_6 = 'commons-cli.version' , '1.4', pom.xml
        # REPLACE_7 = 'avro.version' , ${avro_jar_version}, pom.xml
        REPLACE_8 = 'hive.version' , ${hive_jar_version}, pom.xml
        REPLACE_9 = 'parquet.version' , ${parquet_jar_version}, pom.xml
        REPLACE_10 = 'jackson.databind.version' , '2.9.5', pom.xml
        REPLACE_11 = 'protobuf-java.version' , '2.5.0', pom.xml
        REPLACE_12 = 'libthrift.version' , '0.9.3-1', pom.xml
        REPLACE_13 = 'slf4j.version' , '1.7.25', pom.xml
        REPLACE_14 = 'log4j.version' , '1.2.17', pom.xml
        REPLACE_15 = 'mockito.version' , '1.10.19', pom.xml
        REPLACE_16 = 'netty.version' , '4.1.17.Final', pom.xml
        REPLACE_17 = 'hbase.version' , '${hbase_jar_version}', pom.xml
        REPLACE_18 = 'scala.base.version' , '2.11', pom.xml
        REPLACE_19 = 'scala.version' , '2.11.12', pom.xml
        REPLACE_20 = 'spark.version' , ${spark_jar_version}, pom.xml
        REPLACE_21 = 'jsr305.version' , '3.0.0', pom.xml
        REPLACE_22 = 'zookeeper.version' , ${zookeeper_jar_version}, pom.xml
        REPLACE_23 = 'jline.version', 2.12.1, pom.xml
        REPLACE_24 = 'hadoop.version', ${hadoop_jar_version}, pom.xml

[datafu]
build_tool = gradle

    [[artifacts]]
        artifact_1 = datafu-pig/build/libs/datafu-pig-${datafu_jar_version}.jar
        artifact_2 = datafu-pig/build/libs/datafu-pig-${datafu_jar_version}-core.jar
        artifact_3 = datafu-pig/build/libs/datafu-pig-${datafu_jar_version}-javadoc.jar
        artifact_4 = datafu-pig/build/libs/datafu-pig-${datafu_jar_version}-sources.jar

    [[install_cmd]]
        cmd_0 = ${GRADLE_CMD} -b bootstrap.gradle
        cmd_1 = ./gradlew -Pversion=${datafu_jar_version} -Prelease=true :datafu-pig:assemble install

    [[test_cmd]]
        cmd_1 = ${GRADLE_CMD} -b bootstrap.gradle && ./gradlew -Pversion=${datafu_jar_version} -Prelease=true

    [[text-replace]]
        REPLACE_1 = "datafu-pig-incubating","datafu-pig", datafu-pig/build.gradle , regex_replace

[cdp_data_analytics_studio]
build_tool = maven
setversion_cmd = ${MVN_CMD} ${MVN_SET_VERSION_CMD} -DnewVersion=${cdp_data_analytics_studio_jar_version}
COMMON_BUILD_OPTS = "${MVN_CMD} -Drepoid=${NEXUS_DEPLOY_REPO_ID} -Drepourl=${NEXUS_REPO_URL} -Dreponame=${NEXUS_DEPLOY_REPO_ID}"
deploy_cmd = ${COMMON_BUILD_OPTS} -DskipTests -DskipITs deploy
    [[artifacts]]
        artifact_1 =  ${SOURCE_ROOT}/cdp_data_analytics_studio/web-app/target/cdp-data_analytics_studio-webapp-${cdp_data_analytics_studio_jar_version}.tar.gz
        artifact_2 = ${SOURCE_ROOT}/cdp_data_analytics_studio/event-processor/target/cdp-data_analytics_studio-event-processor-${cdp_data_analytics_studio_jar_version}.tar.gz

    [[install_cmd]]
         cmd_1 = ${COMMON_BUILD_OPTS} -B clean install -Pcdp
         cmd_2 = ${COMMON_BUILD_OPTS} -B -Pcdp -Pvalidate validate

    [[xml-replace]]
        REPLACE_1 = 'hdp3.hive.version', ${hive_jar_version}, pom.xml
        REPLACE_2 = 'hdp3.hadoop.version', ${hadoop_jar_version}, pom.xml
        REPLACE_3 = 'hdp3.tez.version', ${tez_jar_version}, pom.xml
        REPLACE_4 = 'hdp3.ranger.version', ${ranger_jar_version}, pom.xml

[druid]
build_tool = maven
JAVA_HOME=/grid/0/jenkins/tools/jdk8/jdk1.8.0_171
setversion_cmd = ${MVN333_CMD} org.codehaus.mojo:versions-maven-plugin:2.1:set -DgenerateBackupPoms=false -DnewVersion=${druid_jar_version}
deploy_cmd = "${MVN333_CMD} deploy -Drepo.id=${NEXUS_DEPLOY_REPO_ID} -Dinternal.maven.repo=${NEXUS_REPO_URL} -DskipITs -DskipTests"
COMMON_BUILD_OPTS = "${MVN333_CMD} -DskipTests -Ddruid.distribution.pulldeps.opts='-c org.apache.druid.extensions.contrib:dropwizard-emitter' -DrepoOrgUrl=${NEXUS_PROXY_URL}"
coverage_tool = cobertura

    [[artifacts]]
        artifact_1 = distribution/target/apache-druid-${druid_jar_version}-bin.tar.gz

    [[install_cmd]]
        cmd_1 =  ${COMMON_BUILD_OPTS} clean install -Pdist

    [[install_coverage_cmd]]
        cmd_1 =  ${COMMON_BUILD_OPTS} clean

    [[test_cmd]]
        cmd_1 = ${MVN333_CMD} -DfailIfNoTests=false -Djava.net.preferIPv4Stack=true test

    [[test_coverage_cmd]]
        cmd_1 = ${MVN333_CMD} -DfailIfNoTests=false -Djava.net.preferIPv4Stack=true cobertura:cobertura

    [[fortify_cmd]]
        cmd_1 = ${COMMON_BUILD_OPTS} install
        cmd_2 = ${COMMON_BUILD_OPTS} ${FORTIFY_ARGS} -Dfortify.sca.buildId=druid-${druid_jar_version} ${FORTIFY_CLEAN_CMD}
        cmd_3 = ${COMMON_BUILD_OPTS} ${FORTIFY_ARGS} -Dfortify.sca.buildId=druid-${druid_jar_version} -Dfortify.sca.verbose=true -Dfortify.sca.translateLogfile=translate.log -Dfortify.sca.debug=true ${FORTIFY_TRANSLATE_CMD}
        cmd_4 = sourceanalyzer -b druid-${druid_jar_version} ${FORTIFY_SCAN_LARGE_MEMORY} -scan -f druid-${druid_jar_version}.fpr

    [[xml-replace]]
        REPLACE_1 = 'hadoop.compile.version', ${hadoop_jar_version}, pom.xml
        REPLACE_2 = 'zookeeper.version', ${zookeeper_jar_version}, pom.xml
        REPLACE_3 = 'storage-api.version', ${storage_api_jar_version}, extensions-core/druid-bloom-filter/pom.xml

    [[text-replace]]
        REPLACE_1 = '"https://repo1.maven.org/maven2/"', '"https://repo1.maven.org/maven2/", "${NEXUS_PROXY_URL}"', services/src/main/java/org/apache/druid/cli/PullDependencies.java, regex_replace
        REPLACE_2 = '"org.apache.hadoop:hadoop-client:.*"', '"org.apache.hadoop:hadoop-client:${hadoop_jar_version}"', indexing-service/src/main/java/org/apache/druid/indexing/common/config/TaskConfig.java, regex_replace
        REPLACE_3 = 'conf/druid', 'conf', examples/bin/node.sh, regex_replace
        REPLACE_4 = 'var/druid/pids', '/var/run/druid', examples/bin/node.sh, regex_replace

[flume-ng]
build_tool = maven

    [[artifacts]]
        artifact_1 = flume-ng-dist/target/apache-flume-${flume-ng_jar_version}-bin.tar.gz
        artifact_2 = flume-ng-dist/target/apache-flume-${flume-ng_jar_version}-src.tar.gz

    [[setversion_cmd]]
        cmd_1 = ${MVN_CMD} ${MVN_SET_VERSION_CMD} -DnewVersion=${flume-ng_jar_version}
        cmd_2 = ${MVN_CMD} ${MVN_SET_VERSION_CMD} -DnewVersion=${flume-ng_jar_version}, build-support

    [[install_cmd]]
        cmd_1 = ${MVN_CMD} -B -U clean install -DskipTests
        cmd_2 = ${MVN_CMD} source:jar javadoc:javadoc -DskipTests -Psite install

    [[fortify_cmd]]
        cmd_1 = ${MVN_CMD} -DskipTests clean install
        cmd_2 = ${MVN_CMD} ${FORTIFY_ARGS} -Dfortify.sca.buildId=flume-${flume-ng_jar_version} ${FORTIFY_CLEAN_CMD}
        cmd_3 = ${MVN_CMD} -DskipTests ${FORTIFY_ARGS} -Dfortify.sca.buildId=flume-${flume-ng_jar_version} -Dfortify.sca.verbose=true -Dfortify.sca.translateLogfile=translate.log -Dfortify.sca.debug=true ${FORTIFY_TRANSLATE_CMD}
        cmd_4 = sourceanalyzer -b flume-${flume-ng_jar_version} ${FORTIFY_SCAN_MEMORY} -scan -f flume-${flume-ng_jar_version}.fpr

    [[xml-replace]]
        REPLACE_1 = 'scala.binary.version', '2.11', pom.xml
        REPLACE_2 = 'codehaus.jackson.version', '1.9.13', pom.xml
        REPLACE_3 = 'commons-cli.version', '1.4', pom.xml
        REPLACE_4 = 'commons-codec.version', '1.9', pom.xml
        REPLACE_5 = 'commons-collections.version', '3.2.2', pom.xml
        REPLACE_6 = 'commons-compress.version', '1.18', pom.xml
        REPLACE_7 = 'commons-dbcp.version', '1.4', pom.xml
        REPLACE_8 = 'commons-io.version', '2.6', pom.xml
        REPLACE_9 = 'commons-lang.version', '2.6', pom.xml
        REPLACE_10 = 'fasterxml.jackson.version', '2.9.8', pom.xml
        REPLACE_11 = 'hadoop.version', ${hadoop_jar_version}, pom.xml
        REPLACE_12 = 'hbase.version', ${hbase_jar_version}, pom.xml
        # REPLACE_13 = 'hive.version', ${hive_jar_version}, pom.xml
        REPLACE_14 = 'httpclient.version', '4.5.3', pom.xml
        REPLACE_15 = 'httpcore.version', '4.4.6', pom.xml
        REPLACE_16 = 'jetty.version', '9.3.25.v20180904', pom.xml
        REPLACE_17 = 'joda-time.version', '2.9.9', pom.xml
        REPLACE_18 = 'kafka.version', ${kafka_jar_version}, pom.xml
        REPLACE_19 = 'search.version', ${search_jar_version}, pom.xml
        REPLACE_20 = 'netty.version', '3.10.6.Final', pom.xml
        REPLACE_21 = 'snappy-java.version', '1.1.4', pom.xml
        REPLACE_22 = 'solr-global.version', '7.4.0-cdh6.2.x-SNAPSHOT', pom.xml
        REPLACE_23 = 'slf4j.version', '1.7.25', pom.xml
        REPLACE_24 = 'thrift.version', '0.9.3-1', pom.xml
        REPLACE_25 = 'avro.version', ${avro_jar_version}, pom.xml
        REPLACE_26 = 'tika.version', '1.18', pom.xml

[gcs]
build_tool = maven
setversion_cmd = ${MVN_CMD} ${MVN_SET_VERSION_CMD} -DnewVersion=${gcs_jar_version}
deploy_cmd = ${MVN_CMD} deploy -DskipITs -DskipTests -Psite -Dmaven.javadoc.skip=true
package_count = 3
replace_lkgb_version = ${cdh_latest_version}

    [[artifacts]]
        artifact_1 = gcs-${gcs_jar_version}.tar.gz

    [[install_cmd]]
        cmd_1 = ${MVN_HOME}/bin/mvn -B -U clean -Phadoop3 install -DskipTests
        cmd_2 = rm -rf ../gcs-${gcs_jar_version}
        cmd_3 = mkdir ../gcs-${gcs_jar_version}
        cmd_4 = cp -rpf . ../gcs-${gcs_jar_version}
        cmd_5 = tar --exclude-vcs -czf gcs-${gcs_jar_version}.tar.gz ../gcs-${gcs_jar_version}

    [[xml-replace]]
        REPLACE_1 = 'hadoop.three.version' , ${hadoop_lkgb_jar_version} , pom.xml
        REPLACE_2 = 'bigdataoss.version' , ${gcs_jar_version} , pom.xml


[hadoop]
build_tool = maven
COMMON_BUILD_OPTS = "${MVN_CMD} -Drequire.snappy=true -Dbundle.snappy=true -Dsnappy.prefix=x -Dsnappy.lib=${snappylib} -Dbundle.zstd=true -Drequire.zstd=true -Dzstd.lib=${zlib} -Pyarn-ui -Pdist -Pnative -Dtar -Psrc -Pgpg -Drequire.openssl=true -Dmaven.javadoc.skip=true -Dhbase.profile=2.0"
setversion_cmd = ${MVN_CMD} ${MVN_SET_VERSION_CMD} -DnewVersion=${hadoop_jar_version}
deploy_cmd = ${COMMON_BUILD_OPTS} -DdistMgmtStagingId=${NEXUS_DEPLOY_REPO_ID} -DdistMgmtStagingUrl=${NEXUS_REPO_URL} -DskipTests -DskipITs deploy
coverage_tool = cobertura
FORTIFY_VERSION=17.20
FORTIFY_SCA_HOME="${TOOLS_HOME}/fortify_sca_17.20"

    [[artifacts]]
        artifact_1 = hadoop-dist/target/hadoop-${hadoop_jar_version}.tar.gz
        artifact_2 = hadoop-client-modules/hadoop-client/target/hadoop-client-${hadoop_jar_version}.tar.gz
        artifact_3 = hadoop-hdfs-project/hadoop-hdfs/target/hadoop-hdfs-${hadoop_jar_version}.tar.gz
        artifact_4 = hadoop-hdfs-project/hadoop-hdfs-native-client/target/main/native/fuse-dfs/fuse_dfs

    [[download_modules]]
        cmd_1 = ${cdh_S3_DEV_LOC}/tars/isa_l/isal-${isa_l_jar_version}.tar.gz, ${TMP_PACKAGES_DIR}/isal-${isa_l_jar_version}.tar.gz

    [[install_cmd]]
        cmd_1 = tar -zxf ${TMP_PACKAGES_DIR}/isal-${isa_l_jar_version}.tar.gz -C ${TMP_PACKAGES_DIR}
        cmd_2 = ${COMMON_BUILD_OPTS} -DdistMgmtStagingId=${NEXUS_DEPLOY_REPO_ID} -DdistMgmtStagingUrl=${NEXUS_REPO_URL} -Dcontainer-executor.additional_cflags='-std=c90' -DskipTests -DskipITs -Drequire.isal -Disal.lib=${TMP_PACKAGES_DIR}/isal-${isa_l_jar_version}/lib install
        cmd_3 = ${COMMON_BUILD_OPTS} -DdistMgmtStagingId=${NEXUS_DEPLOY_REPO_ID} -DdistMgmtStagingUrl=${NEXUS_REPO_URL} -DskipTests -DskipITs site:site

    [[fortify_cmd]]
        cmd_1 = tar -zxf ${TMP_PACKAGES_DIR}/isal-${isa_l_jar_version}.tar.gz -C ${TMP_PACKAGES_DIR}
        cmd_2 = ${COMMON_BUILD_OPTS} -DdistMgmtStagingId=${NEXUS_DEPLOY_REPO_ID} -DdistMgmtStagingUrl=${NEXUS_REPO_URL} -Dcontainer-executor.additional_cflags='-std=c90' -DskipTests -DskipITs -Drequire.isal -Disal.lib=${TMP_PACKAGES_DIR}/isal-${isa_l_jar_version}/lib install
        cmd_3 = ${COMMON_BUILD_OPTS} -DdistMgmtStagingId=${NEXUS_DEPLOY_REPO_ID} -DdistMgmtStagingUrl=${NEXUS_REPO_URL} -Dcontainer-executor.additional_cflags='-std=c90' -DskipTests -DskipITs -Drequire.isal -Disal.lib=${TMP_PACKAGES_DIR}/isal-${isa_l_jar_version}/lib -Dfortify.sca.Xmx=48G -Dfortify.sca.Xss=400M -Dfortify.sca.sourceanalyzer.executable=${TOOLS_HOME}/fortify_sca_17.20/bin/sourceanalyzer -Dfortify.sca.source.version=1.8 -Dfortify.sca.cp=${MR_HOME}/repository -Dfortify.sca.buildId=hadoop-${hadoop_jar_version} ${FORTIFY_CLEAN_CMD}
        cmd_4 = ${COMMON_BUILD_OPTS} -DdistMgmtStagingId=${NEXUS_DEPLOY_REPO_ID} -DdistMgmtStagingUrl=${NEXUS_REPO_URL} -Dcontainer-executor.additional_cflags='-std=c90' -DskipTests -DskipITs -Drequire.isal -Disal.lib=${TMP_PACKAGES_DIR}/isal-${isa_l_jar_version}/lib -Dfortify.sca.Xmx=48G -Dfortify.sca.Xss=400M -Dfortify.sca.sourceanalyzer.executable=${TOOLS_HOME}/fortify_sca_17.20/bin/sourceanalyzer -Dfortify.sca.source.version=1.8 -Dfortify.sca.cp=${MR_HOME}/repository -Dfortify.sca.buildId=hadoop-${hadoop_jar_version} -Dfortify.sca.verbose=true -Dfortify.sca.translateLogfile=translate.log -Dfortify.sca.debug=true ${FORTIFY_TRANSLATE_CMD}
    	cmd_5 = sourceanalyzer -b hadoop-${hadoop_jar_version} ${FORTIFY_SCAN_LARGE_MEMORY} -scan -f hadoop-${hadoop_jar_version}.fpr

    [[test_cmd]]
        cmd_1 = ${MVN_CMD} test

    [[test_coverage_cmd]]
        cmd_1 = ${MVN_CMD} cobertura:cobertura

    [[text-replace]]
        REPLACE_1 = "\$\{cdh.logredactor.version\}", "2.0.7", hadoop-common-project/hadoop-common/pom.xml, regex_replace

    [[xml-replace]]
        REPLACE_1 = 'forkedProcessTimeoutInSeconds', '1800' , hadoop-project/pom.xml
        REPLACE_2 = 'forkedProcessTimeoutInSeconds', '1800' , hadoop-tools/hadoop-distcp/pom.xml
        REPLACE_3 = 'argLine', '-Xms2048m -Xmx3072m -XX:MaxPermSize=2048m -XX:+HeapDumpOnOutOfMemoryError', hadoop-project/pom.xml
        REPLACE_4 = 'argLine', '-Xms2048m -Xmx3072m -XX:MaxPermSize=2048m -XX:+HeapDumpOnOutOfMemoryError', hadoop-tools/hadoop-distcp/pom.xml
        REPLACE_5 = 'zookeeper.version', ${zookeeper_jar_version} , pom.xml
        REPLACE_7 = 'gcs.version' , ${gcs_jar_version} , hadoop-project/pom.xml
        REPLACE_8 = 'hadoop.version', ${hadoop_jar_version}, pom.xml
        REPLACE_9 = 'hadoop.assemblies.version', ${hadoop_jar_version}, hadoop-project/pom.xml
        REPLACE_10 = 'knox.version', ${knox_lkgb_jar_version} , hadoop-project/pom.xml

[ozone]
build_tool = maven
COMMON_BUILD_OPTS = "${MVN_CMD} clean install -Pdist -DskipTests -Dmaven.javadoc.skip=true"
deploy_cmd = ${COMMON_BUILD_OPTS} -DdistMgmtStagingId=${NEXUS_DEPLOY_REPO_ID} -DdistMgmtStagingUrl=${NEXUS_REPO_URL} -DskipTests -DskipITs deploy
coverage_tool = cobertura

    [[artifacts]]
        artifact_1 = hadoop-ozone/dist/target/hadoop-ozone-${ozone_jar_version}.tar.gz

    [[setversion_cmd]]
        cmd_1 = ${MVN_CMD} ${MVN_SET_VERSION_CMD} -DnewVersion=${ozone_jar_version}

    [[install_cmd]]
        cmd_1 = ${COMMON_BUILD_OPTS} -DskipTests -DskipITs -DdistMgmtStagingId=${NEXUS_DEPLOY_REPO_ID} -DdistMgmtStagingUrl=${NEXUS_REPO_URL}

    [[fortify_cmd]]
        cmd_1 = ${COMMON_BUILD_OPTS} -DskipTests -DskipITs -DdistMgmtStagingId=${NEXUS_DEPLOY_REPO_ID} -DdistMgmtStagingUrl=${NEXUS_REPO_URL}
        cmd_3 = ${COMMON_BUILD_OPTS} -DskipTests -DskipITs -DdistMgmtStagingId=${NEXUS_DEPLOY_REPO_ID} -DdistMgmtStagingUrl=${NEXUS_REPO_URL} ${FORTIFY_ARGS} -Dfortify.sca.buildId=ozone-${ozone_jar_version} ${FORTIFY_CLEAN_CMD}
        cmd_4 = ${COMMON_BUILD_OPTS} -DskipTests -DskipITs -DdistMgmtStagingId=${NEXUS_DEPLOY_REPO_ID} -DdistMgmtStagingUrl=${NEXUS_REPO_URL} ${FORTIFY_ARGS} -Dfortify.sca.buildId=ozone-${ozone_jar_version} -Dfortify.sca.verbose=true -Dfortify.sca.translateLogfile=translate.log -Dfortify.sca.debug=true ${FORTIFY_TRANSLATE_CMD}
        cmd_5 = sourceanalyzer -b ozone-${ozone_jar_version} ${FORTIFY_SCAN_LARGE_MEMORY} -scan -f ozone-${ozone_jar_version}.fpr

    [[test_cmd]]
        cmd_1 = ${MVN_CMD} test

    [[test_coverage_cmd]]
        cmd_1 = ${MVN_CMD} cobertura:cobertura

    [[xml-replace]]
        REPLACE_1 = 'zookeeper.version', ${zookeeper_jar_version} , pom.xml
        REPLACE_2 = 'hadoop.version', ${hadoop_jar_version}, pom.xml
        REPLACE_3 = 'hdds.version' , ${ozone_jar_version} , pom.xml
        REPLACE_4 = 'ozone.version' , ${ozone_jar_version} , hadoop-ozone/pom.xml
        #REPLACE_5 = 'hadoop.version' , ${hadoop_jar_version} , hadoop-ozone/pom.xml
        REPLACE_6 = 'ratis.version' , ${ratis_jar_version} , hadoop-hdds/pom.xml
        REPLACE_7 = 'ratis.version' , ${ratis_jar_version} , hadoop-ozone/pom.xml

[hadoop-lzo]
build_tool = maven
setversion_cmd = ${MVN_CMD} ${MVN_SET_VERSION_CMD} -DnewVersion=${hadoop-lzo_jar_version}

    [[artifacts]]
        artifact_1 = target/hadoop-lzo-${hadoop-lzo_jar_version}.tar.gz

    [[install_cmd]]
        cmd_1 = "${MVN_CMD} -Dversion=${hadoop-lzo_jar_version} -Dhadoop.version=${hadoop_jar_version} -Drepo.maven.org=${NEXUS_REPO_URL} -Dmvnrepo=${NEXUS_REPO_URL} clean install package -Dmaven.javadoc.skip=true"
        cmd_2 = "tar -zvcf hadoop-lzo-${hadoop-lzo_jar_version}.tar.gz native hadoop-lzo-${hadoop-lzo_jar_version}.jar  hadoop-lzo-${hadoop-lzo_jar_version}-sources.jar --exclude=native/Linux-amd64-64/src --exclude=native/Linux-amd64-64/.libs --exclude=native/Linux-amd64-64/impl --exclude=native/Linux-amd64-64/libgplcompression.la --exclude=native/Linux-amd64-64/Makefile --exclude=native/Linux-amd64-64/config.status --exclude=native/Linux-amd64-64/config.log --exclude=native/Linux-amd64-64/libtool" , target

    [[xml-replace]]
        REPLACE_1 = 'hadoop.current.version', ${hadoop_jar_version} , pom.xml

[hbase]
build_tool = maven
COMMON_BUILD_OPTS = "${MVN_CMD} -Drepoid=${NEXUS_DEPLOY_REPO_ID} -Drepourl=${NEXUS_REPO_URL} -Dhadoop.profile=3.0 clean"
setversion_cmd = ${MVN_CMD} ${MVN_SET_VERSION_CMD} -DnewVersion=${hbase_jar_version}
deploy_cmd = ${COMMON_BUILD_OPTS} deploy -DskipTests=true -Dmaven.javadoc.skip=true
coverage_tool = cobertura
FORTIFY_VERSION=17.20
FORTIFY_SCA_HOME="${TOOLS_HOME}/fortify_sca_17.20"

    [[artifacts]]
        artifact_1 = hbase-assembly/target/hbase-${hbase_jar_version}-bin.tar.gz
        artifact_2 = hbase-thrift/src/main/resources/org/apache/hadoop/hbase/thrift/Hbase.thrift
        artifact_3 = hbase-thrift/src/main/resources/org/apache/hadoop/hbase/thrift2/hbase.thrift

    [[install_cmd]]
        cmd_1 = ${COMMON_BUILD_OPTS} install assembly:single -DskipTests=true

    [[fortify_cmd]]
        cmd_1 = ${COMMON_BUILD_OPTS} install assembly:single -DskipTests=true
        cmd_2 = ${COMMON_BUILD_OPTS} -DskipTests=true -Dfortify.sca.Xmx=48G -Dfortify.sca.Xss=400M -Dfortify.sca.sourceanalyzer.executable=${TOOLS_HOME}/fortify_sca_17.20/bin/sourceanalyzer -Dfortify.sca.source.version=1.8 -Dfortify.sca.cp=${MR_HOME}/repository -Dfortify.sca.buildId=hbase-${hbase_jar_version} ${FORTIFY_CLEAN_CMD}
        cmd_3 = ${COMMON_BUILD_OPTS} -DskipTests=true -Dfortify.sca.Xmx=48G -Dfortify.sca.Xss=400M -Dfortify.sca.sourceanalyzer.executable=${TOOLS_HOME}/fortify_sca_17.20/bin/sourceanalyzer -Dfortify.sca.source.version=1.8 -Dfortify.sca.cp=${MR_HOME}/repository -Dfortify.sca.buildId=hbase-${hbase_jar_version} ${FORTIFY_TRANSLATE_CMD}
        cmd_4 = sourceanalyzer -b hbase-${hbase_jar_version} ${FORTIFY_SCAN_MEMORY} -scan -f hbase-${hbase_jar_version}.fpr

    # "package" is the recommended lifecycle phase for running unit tests for Maven projects.
    # Also, MAWO may choose to run UTs that span Maven modules. Without the fail-never option, the failure
    # of a UT in an earlier module may preclude execution of the tests in a later module.
   [[test_cmd]]
        cmd_1 = ${MVN_CMD} --fail-never -Phadoop-3.0 package

   [[test_coverage_cmd]]
        cmd_1 = ${MVN_CMD} --fail-never -Phadoop-3.0 cobertura:cobertura

    [[xml-replace]]
        REPLACE_1 = 'hadoop-one.version',${hadoop_jar_version}, pom.xml
        REPLACE_2 = 'hadoop-two.version', ${hadoop_jar_version}, pom.xml
        REPLACE_3 = 'hadoop-three.version', ${hadoop_jar_version}, pom.xml
        REPLACE_4 = 'zookeeper.version', ${zookeeper_jar_version}, pom.xml
        REPLACE_5 = 'surefire.firstPartThreadCount', '1', pom.xml
        REPLACE_6 = 'surefire.secondPartThreadCount', '1', pom.xml
        REPLACE_7 = 'surefire.timeout', '7200', pom.xml
        REPLACE_8 = 'activeByDefault' , 'false' , pom.xml
        REPLACE_9 = 'hbase.version' , ${hbase_jar_version} , pom.xml
        REPLACE_10 = 'spark.version' , ${spark_lkgb_jar_version} , pom.xml

[hbase_connectors]
build_tool = maven
COMMON_BUILD_OPTS = "${MVN352_CMD} -Dhadoop.profile=3.0 clean"
setversion_cmd = ${MVN352_CMD} ${MVN_SET_VERSION_CMD} -DnewVersion=${hbase_connectors_jar_version}
deploy_cmd = ${COMMON_BUILD_OPTS} deploy -DskipTests=true -Dmaven.javadoc.skip=true
coverage_tool = cobertura

    [[artifacts]]
        artifact_1 = hbase-connectors-assembly/target/hbase-connectors-assembly-${hbase_connectors_jar_version}-bin.tar.gz

    [[install_cmd]]
        cmd_1 = ${COMMON_BUILD_OPTS} install -DskipTests=true

   [[test_cmd]]
        cmd_1 = ${MVN352_CMD} --fail-never package

    [[xml-replace]]
        REPLACE_1 = 'hbase.version', '${hbase_jar_version}', pom.xml
        REPLACE_2 = 'revision', assembly-${hbase_connectors_jar_version}, pom.xml
	REPLACE_3 = 'hadoop-two.version', ${hadoop_jar_version}, pom.xml
        REPLACE_4 = 'hadoop-three.version', ${hadoop_jar_version}, pom.xml
        REPLACE_5 = 'avro.version', ${avro_jar_version}, kafka/pom.xml
        REPLACE_6 = 'kafka-clients.version', ${kafka_jar_version}, kafka/hbase-kafka-proxy/pom.xml
        REPLACE_7 = 'hbase-thirdparty.version', '2.2.0', spark/pom.xml
        REPLACE_8 = 'scala.version', '2.11.12', spark/pom.xml
        REPLACE_9 = 'spark.version', ${spark_jar_version} , spark/pom.xml
        REPLACE_10 = 'spark.scala.binary.version', '2.11', spark/pom.xml

[hbase_filesystem]
build_tool = maven
COMMON_BUILD_OPTS = "${MVN352_CMD} clean"
setversion_cmd = ${MVN352_CMD} ${MVN_SET_VERSION_CMD} -DnewVersion=${hbase_filesystem_jar_version}
deploy_cmd = ${COMMON_BUILD_OPTS} deploy -DskipTests=true -Dmaven.javadoc.skip=true
coverage_tool = cobertura

    [[artifacts]]
        artifact_1 = dist/target/hbase-filesystem-dist-${hbase_filesystem_jar_version}.tar.gz

    [[install_cmd]]
        cmd_1 = ${COMMON_BUILD_OPTS} install -DskipTests=true

   [[test_cmd]]
        cmd_1 = ${MVN352_CMD} --fail-never package

    [[xml-replace]]
        REPLACE_1 = 'hbase.version', ${hbase_jar_version}, pom.xml
        REPLACE_2 = 'hadoop.version',${hadoop_jar_version}, pom.xml
        REPLACE_3 = 'zookeeper.version', ${zookeeper_jar_version}, pom.xml

[hbase-solr]
build_tool = maven
setversion_cmd = ${MVN_CMD} ${MVN_SET_VERSION_CMD} -DnewVersion=${hbase-solr_jar_version}

    [[artifacts]]

    artifact_1 = hbase-indexer-dist/target/hbase-indexer-${hbase-solr_jar_version}.tar.gz

    [[install_cmd]]
        cmd_1 = ${MVN_CMD} -pl . --also-make  -Dmaven.javadoc.skip=true -Dfindbugs.skip=true -DskipTests clean install
        cmd_2 = ${MVN_CMD} -Pdist -Dhbase.api=2.0 -DskipTests install

    [[xml-replace]]
        REPLACE_1 = 'version.solr', ${solr_jar_version}, pom.xml
        REPLACE_2 = 'version.guava', '11.0.2', pom.xml
        REPLACE_3 = 'version.joda-time', '2.9.9', pom.xml
        REPLACE_4 = 'version.slf4j', '1.7.25', pom.xml
        REPLACE_5 = 'version.hbase', ${hbase_jar_version}, pom.xml
        REPLACE_6 = 'version.hadoop', ${hadoop_jar_version}, pom.xml
        REPLACE_7 = 'version.zookeeper', ${zookeeper_jar_version}, pom.xml
        REPLACE_8 = 'version.jackson', '1.9.13', pom.xml
        REPLACE_9 = 'version.jackson-mapper-asl', '1.9.13-cloudera.1', pom.xml
        REPLACE_10 = 'version.jackson-hadoop', '2.10.0', pom.xml
        REPLACE_11 = 'version.jetty', '9.3.27.v20190418', pom.xml
        REPLACE_12 = 'version.httpclient', '4.5.3', pom.xml
        REPLACE_13 = 'version.httpcore', '4.4.6', pom.xml
        REPLACE_15 = 'version.jersey', '1.19', pom.xml
        REPLACE_16 = 'version.jaxb-api', '2.2.11', pom.xml
        REPLACE_18 = 'version.solr-security-util', ${solr_jar_version}, pom.xml
        REPLACE_19 = 'version.netty', '4.1.17.Final', pom.xml
        REPLACE_20 = 'version.netty.hadoop', '3.10.5.Final', pom.xml
        REPLACE_21 = 'version.surefire.plugin', '2.20', pom.xml
        REPLACE_22 = 'version.search', ${search_jar_version}, pom.xml

[storage_api]
build_tool = maven
COMMON_BUILD_OPTS = "${MVN_CMD}"
setversion_cmd = ${MVN_CMD} ${MVN_SET_VERSION_CMD} -DnewVersion=${storage_api_jar_version} , storage-api
deploy_cmd = "${COMMON_BUILD_OPTS} clean deploy -DskipTests" , storage-api

    [[install_cmd]]
        cmd_1 = "${COMMON_BUILD_OPTS} clean  install -DskipTests -Dmaven.javadoc.skip=true -Denforcer.skip=true" , storage-api

    [[test_cmd]]
        cmd_1 = "${COMMON_BUILD_OPTS} test-compile -Dmaven.test.failure.ignore=true" , storage-api

    [[xml-replace]]
        REPLACE_1 = 'hadoop.version', ${hadoop_jar_version}, pom.xml

[oozie]
build_tool = maven
COMMON_BUILD_OPTS = "${MVN_CMD} -fae -Dhadoop.version=${hadoop_jar_version} -Dhive.version=${hive_jar_version} -Dpig.version=${pig_jar_version} -Drepoid=${NEXUS_DEPLOY_REPO_ID} -Dreponame=${NEXUS_DEPLOY_REPO_ID} -Drepourl=${NEXUS_REPO_URL} -DdistMgmtReleaseUrl=${NEXUS_REPO_URL} -DmavenReleaseId=${NEXUS_DEPLOY_REPO_ID} -Puber,cloud"
setversion_cmd = ${MVN_CMD} ${MVN_SET_VERSION_CMD} -DnewVersion=${oozie_jar_version}
deploy_cmd = ${COMMON_BUILD_OPTS} deploy -DskipITs -DskipTests=true
coverage_tool = cobertura

    [[artifacts]]
        artifact_1 = distro/target/oozie-${oozie_jar_version}-distro.tar.gz
        artifact_2 = LICENSE.txt
        artifact_3 = NOTICE.txt
        artifact_4 = README.md
        artifact_5 = release-log.txt
        artifact_6 = source-headers.txt

    [[install_cmd]]
        cmd_1 = ${COMMON_BUILD_OPTS} clean install assembly:single -DskipITs -DskipTests=true -DgenerateDocs

    [[test_cmd]]
        cmd_1 = ${COMMON_BUILD_OPTS} ${MAVEN_TEST_OPTS} -Djava.net.preferIPv4Stack=true test

    [[test_coverage_cmd]]
        cmd_1 = ${COMMON_BUILD_OPTS} ${MAVEN_TEST_OPTS} -Djava.net.preferIPv4Stack=true cobertura:cobertura

    [[fortify_cmd]]
        cmd_1 = ${COMMON_BUILD_OPTS} -DskipITs -DskipTests clean install
        cmd_2 = ${COMMON_BUILD_OPTS} -DskipITs -DskipTests ${FORTIFY_ARGS} -Dfortify.sca.buildId=oozie-${oozie_jar_version} ${FORTIFY_CLEAN_CMD}
        cmd_3 = ${COMMON_BUILD_OPTS} -DskipITs -DskipTests ${FORTIFY_ARGS} -Dfortify.sca.buildId=oozie-${oozie_jar_version} ${FORTIFY_TRANSLATE_CMD}
    	cmd_4 = sourceanalyzer -b oozie-${oozie_jar_version} ${FORTIFY_SCAN_MEMORY} -scan -f oozie-${oozie_jar_version}.fpr

    [[xml-replace]]
        REPLACE_1 = 'hbase.version' , ${hbase_jar_version} , pom.xml
        REPLACE_2 = 'hbaselib.version' , ${hbase_jar_version}.oozie-${oozie_jar_version} , pom.xml
        REPLACE_3 = 'hcatalog.version' , ${hive_jar_version} , pom.xml
        REPLACE_4 = 'sqoop.version' , ${sqoop_jar_version} , pom.xml
        REPLACE_5 = 'pig.version' , ${pig_jar_version} , pom.xml
        REPLACE_6 = 'hive.version' , ${hive_jar_version} , pom.xml
        REPLACE_7 = 'tez.version' , ${tez_jar_version} , pom.xml
        REPLACE_8 = 'hadoop.auth.version' , ${hadoop_jar_version} , pom.xml
        REPLACE_9 = 'hadoopTwo.version' , ${hadoop_jar_version} , pom.xml
        REPLACE_11 = 'activeByDefault' , 'false' , pom.xml
        REPLACE_12 = 'hadoop.version' , ${hadoop_jar_version} , pom.xml
        REPLACE_13 = 'spark.version' , ${spark_jar_version} , pom.xml
        REPLACE_14 = 'gcs.version' , ${gcs_jar_version} , pom.xml
        REPLACE_15 = 'spark.streaming.kafka.version' , ${spark_jar_version} , pom.xml
        REPLACE_16 = 'spark.bagel.version' , ${spark_jar_version} , pom.xml
        REPLACE_18 = 'avro.version' , ${avro_jar_version} , pom.xml
        REPLACE_19 = 'parquet.version' , ${parquet_jar_version} , pom.xml
        REPLACE_20 = 'spark.hive2.version' , ${spark_hive2_jar_version} , pom.xml
        REPLACE_21 = 'spark.atlas.connector.version' , ${spark_atlas_connector_jar_version} , pom.xml

[orc]
build_tool = maven
COMMON_BUILD_OPTS = "${MVN_CMD}"
setversion_cmd = ${MVN_CMD} ${MVN_SET_VERSION_CMD} -DnewVersion=${orc_jar_version} , java
deploy_cmd = "${COMMON_BUILD_OPTS} clean deploy -DskipTests" , java

    [[install_cmd]]
        cmd_1 = "${COMMON_BUILD_OPTS} clean  install -DskipTests -Dmaven.javadoc.skip=true -Denforcer.skip=true" , java

    [[test_cmd]]
        cmd_1 = "${COMMON_BUILD_OPTS} test-compile -Dmaven.test.failure.ignore=true" , java

    [[xml-replace]]
        REPLACE_1 = 'hadoop.version', ${hadoop_jar_version}, pom.xml
        REPLACE_2 = 'storage-api.version', ${storage_api_jar_version}, pom.xml
        REPLACE_3 = 'zookeeper.version', ${zookeeper_jar_version}, pom.xml

[hive]
build_tool = maven
hive_alternate_name = apache-hive
hive_apache_base_maven_version = 1.2.1
COMMON_BUILD_OPTS = "${MVN_CMD} -Dhadoop.mr.rev=23 -DskipSparkTests -Drepo.maven.org=${NEXUS_PROXY_URL} -Dtest.junit.output.format=xml -Dmvn.hadoop.profile=hadoop23 -Drepoid=${NEXUS_DEPLOY_REPO_ID} -Drepourl=${NEXUS_REPO_URL} -Dmvnrepo=${NEXUS_PROXY_URL}"
deploy_cmd = "${COMMON_BUILD_OPTS} clean deploy -DskipTests -Phadoop-2,dist,sources"
coverage_tool = cobertura
FORTIFY_VERSION=17.20
FORTIFY_SCA_HOME="${TOOLS_HOME}/fortify_sca_17.20"

    [[artifacts]]
        artifact_1 = packaging/target/${hive_alternate_name}-${hive_jar_version}-bin.tar.gz

    [[setversion_cmd]]
        cmd_1 = ${MVN_CMD} ${MVN_SET_VERSION_CMD} -DnewVersion=${hive_jar_version}
        cmd_2 = ${MVN_CMD} ${MVN_SET_VERSION_CMD} -DnewVersion=${hive_jar_version} , storage-api
        cmd_3 = ${MVN_CMD} ${MVN_SET_VERSION_CMD} -DnewVersion=${hive_jar_version}, standalone-metastore
        cmd_4 = ${MVN_CMD} ${MVN_SET_VERSION_CMD} -DnewVersion=${hive_jar_version}, upgrade-acid

    [[install_cmd]]
        cmd_1 = "${COMMON_BUILD_OPTS} clean install -DskipTests -Pdist,sources,re-shaded -Dmaven.javadoc.skip=true -Denforcer.skip=true"

    [[fortify_cmd]]
        cmd_1 = "${COMMON_BUILD_OPTS} clean  install -DskipTests -Pdist,sources -Dmaven.javadoc.skip=true -Denforcer.skip=true"
        cmd_2 = "${COMMON_BUILD_OPTS} -DskipTests -Pdist,sources -Dmaven.javadoc.skip=true -Denforcer.skip=true -Dfortify.sca.Xmx=48G -Dfortify.sca.Xss=400M -Dfortify.sca.sourceanalyzer.executable=${TOOLS_HOME}/fortify_sca_17.20/bin/sourceanalyzer -Dfortify.sca.source.version=1.8 -Dfortify.sca.cp=${MR_HOME}/repository -Dfortify.sca.buildId=hive-${hive_jar_version} ${FORTIFY_CLEAN_CMD}"
        cmd_3 = "${COMMON_BUILD_OPTS} -DskipTests -Pdist,sources -Dmaven.javadoc.skip=true -Denforcer.skip=true -Dfortify.sca.Xmx=48G -Dfortify.sca.Xss=400M -Dfortify.sca.sourceanalyzer.executable=${TOOLS_HOME}/fortify_sca_17.20/bin/sourceanalyzer -Dfortify.sca.source.version=1.8 -Dfortify.sca.cp=${MR_HOME}/repository -Dfortify.sca.buildId=hive-${hive_jar_version} -Dfortify.sca.verbose=true -Dfortify.sca.translateLogfile=translate.log -Dfortify.sca.debug=true ${FORTIFY_TRANSLATE_CMD}"
    	cmd_4 = sourceanalyzer -b hive-${hive_jar_version} ${FORTIFY_SCAN_LARGE_MEMORY} -scan -f hive-${hive_jar_version}.fpr

    [[test_cmd]]
        cmd_1 = "${COMMON_BUILD_OPTS} test-compile -Dmaven.test.failure.ignore=true"

    [[test_coverage_cmd]]
        cmd_1 = "${COMMON_BUILD_OPTS} cobertura:cobertura -Dmaven.test.failure.ignore=true"

    [[text-replace]]
        REPLACE_1 = "${hive_apache_base_maven_version}-SNAPSHOT", ${hive_jar_version}, pom.xml , key_value
        REPLACE_2 = "${hive_apache_base_maven_version}", ${hive_jar_version}, pom.xml , key_value

    [[xml-replace]]
        REPLACE_1 = 'accumulo.version', ${accumulo_jar_version}, pom.xml
        REPLACE_2 = 'calcite.version', ${calcite_jar_version}, pom.xml
        REPLACE_3 = 'hadoop.version', ${hadoop_jar_version}, pom.xml
        REPLACE_4 = 'hbase.version', ${hbase_jar_version}, pom.xml
        REPLACE_5 = 'hbase.hadoop2.version', ${hbase_jar_version}, pom.xml
        REPLACE_6 = 'tez.version', ${tez_jar_version}, pom.xml
        REPLACE_8 = 'zookeeper.version', ${zookeeper_jar_version}, pom.xml
        REPLACE_10 = 'storage-api.version', ${storage_api_jar_version}, pom.xml
        REPLACE_11 = 'orc.version', ${orc_jar_version}, pom.xml
        REPLACE_12 = 'druid.version', ${druid_jar_version}, pom.xml
        REPLACE_13 = 'arrow.version', ${arrow_jar_version}, pom.xml
        REPLACE_14 = 'parquet.version', ${parquet_jar_version}, pom.xml
        REPLACE_15 = 'kafka.version', ${kafka_jar_version}, kafka-handler/pom.xml
        REPLACE_16 = 'avatica.version', ${avatica_jar_version}, pom.xml

[isa_l]

    [[artifacts]]
        artifact_1 = isal-${isa_l_jar_version}.tar.gz

    [[install_cmd]]
        cmd_1 = sh autogen.sh
        cmd_2 = ./configure --prefix=${SOURCE_ROOT}/isa_l/isal-${isa_l_jar_version}
        cmd_3 = make
        cmd_4 = make install
        cmd_5 = tar -zcf isal-${isa_l_jar_version}.tar.gz isal-${isa_l_jar_version}

[impala]
PATH = "${SOURCE_ROOT}/buildvenv2.7/bin:${PATH}"
DOCKER_REGISTRY = "docker-private.infra.cloudera.com/cdh/"

  [[artifacts]]
    artifact_1 = impala-${impala_jar_version}.tar.gz
    # Note: kudu-for-impala-${kudu_jar_version}.tar.gz unpacks to kudu-${kudu_jar_version}/
    # The kudu-for-impala name is to keep it obviously distinct from the regular Kudu build.
    artifact_2 = kudu-for-impala-${kudu_jar_version}.tar.gz
    artifact_3 = kudu-jars-${kudu_jar_version}.tar.gz
    artifact_4 = ccache-log-kudu-for-impala.txt
    artifact_5 = ccache-log-impala-build.txt

  [[download_modules]]
    cmd_1 = ${cdh_S3_DEV_LOC}/tars/hive/hive-${hive_jar_version}-source.tar.gz, ${TMP_PACKAGES_DIR}/hive-${hive_jar_version}-source.tar.gz
    cmd_2 = ${cdh_S3_DEV_LOC}/tars/hadoop/hadoop-${hadoop_jar_version}.tar.gz , ${TMP_PACKAGES_DIR}/hadoop-${hadoop_jar_version}.tar.gz
    cmd_3 = ${cdh_S3_DEV_LOC}/tars/kudu/kudu-${kudu_jar_version}-source.tar.gz , ${TMP_PACKAGES_DIR}/kudu-${kudu_jar_version}-source.tar.gz
    cmd_4 = ${cdh_S3_DEV_LOC}/tars/kudu/kudu-${kudu_jar_version}.tar.gz , ${TMP_PACKAGES_DIR}/kudu-${kudu_jar_version}.tar.gz

  [[install_cmd]]
    cmd_1 = virtualenv --python=${HOME}/tools/python/2.7.9/bin/python buildvenv2.7 , ${SOURCE_ROOT}
    cmd_2 = bash -c "TMP_PACKAGES_DIR=${TMP_PACKAGES_DIR} IMPALA_VERSION=${impala_jar_version} ./cloudera/cdp_install_cmd.sh"

  [[text-replace]]
    # The text replacements are used in two different ways:
    # 1. Text replacements are used when doing a downstream CDP build to set versions in
    #    the source so that the Impala build knows what versions it is building against.
    #    The modifications are used only by the build system and do not get committed.
    # 2. Text replacements are also used by the set_snapshot_versions job to set SNAPSHOT
    #    versions. The job performs the text replacements and it commits any changes.
    #    Since the SNAPSHOT versions do not contain the build number, the versions should
    #    not change frequently. This job keeps Impala up to date when branching.
    # To handle these two cases, there are some rules for our text replacements:
    # - All text replacements should be idempotent.
    # - All text replacements should change infrequently when running via the
    #   set_snapshot_versions job. This rules out things like GBNs.
    # - Since Impala does not use SNAPSHOT versions for the standalone build, the
    #   snapshot replacements happen in a location that only impacts the downstream CDP
    #   build and does not impact a standalone build.

    # Set the docker registry. This must be REPLACE_1 to line up with canary.ini's REPLACE_1
    REPLACE_1 = "__REGISTRY__", ${DOCKER_REGISTRY}, cloudera/docker_images.yml, regex_replace

    # Set the versions in bin/impala-config-branch.sh. These do not impact the standalone
    # Impala build. They are only used for the downstream CDP build.
    REPLACE_2 = "export CDP_HADOOP_VERSION=.*", 'export CDP_HADOOP_VERSION=${hadoop_jar_version}', bin/impala-config-branch.sh, regex_replace
    REPLACE_3 = "export CDP_HBASE_VERSION=.*", 'export CDP_HBASE_VERSION=${hbase_jar_version}', bin/impala-config-branch.sh, regex_replace
    REPLACE_4 = "export CDP_HIVE_VERSION=.*", 'export CDP_HIVE_VERSION=${hive_jar_version}', bin/impala-config-branch.sh, regex_replace
    REPLACE_5 = "export CDP_KNOX_VERSION=.*", 'export CDP_KNOX_VERSION=${knox_jar_version}', bin/impala-config-branch.sh, regex_replace
    REPLACE_6 = "export CDP_KUDU_VERSION=.*", 'export CDP_KUDU_VERSION=${kudu_jar_version}', bin/impala-config-branch.sh, regex_replace
    REPLACE_7 = "export CDP_KUDU_JAVA_VERSION=.*", 'export CDP_KUDU_JAVA_VERSION=${kudu_jar_version}', bin/impala-config-branch.sh, regex_replace
    REPLACE_8 = "export CDP_RANGER_VERSION=.*", 'export CDP_RANGER_VERSION=${ranger_jar_version}', bin/impala-config-branch.sh, regex_replace
    REPLACE_9 = "export CDP_TEZ_VERSION=.*", 'export CDP_TEZ_VERSION=${tez_jar_version}', bin/impala-config-branch.sh, regex_replace
    # Until Impala uses CDP_KUDU_VERSION/CDP_KUDU_JAVA_VERSION, we also need to override
    # IMPALA_KUDU_VERSION and IMPALA_KUDU_JAVA_VERSION as well.
    # TODO: Remove these when Impala fixes its CDP Kudu handling
    REPLACE_10 = "export IMPALA_KUDU_VERSION=.*", 'export IMPALA_KUDU_VERSION=${kudu_jar_version}', bin/impala-config-branch.sh, regex_replace
    REPLACE_11 = "export IMPALA_KUDU_JAVA_VERSION=.*", 'export IMPALA_KUDU_JAVA_VERSION=${kudu_jar_version}', bin/impala-config-branch.sh, regex_replace

    # This is the only replacement that impacts the standalone Impala build. This gets
    # updated on branching and is used to get an appropriate GBN from builddb.
    REPLACE_12 = "export CDP_VERSION=.*", 'export CDP_VERSION=${STACKVERSION}', bin/impala-config-branch.sh, regex_replace

  [[docker_metadata_cmd]]
        cmd_1 = cloudera/docker_images.yml, file

  [[fortify_cmd]]
    cmd_1 = virtualenv --python=${HOME}/tools/python/2.7.9/bin/python buildvenv2.7 , ${SOURCE_ROOT}
    cmd_2 = bash -c "TMP_PACKAGES_DIR=${TMP_PACKAGES_DIR} IMPALA_VERSION=${impala_jar_version} ./cloudera/cdp_install_cmd.sh"
    cmd_3 = bash -c " . ${SOURCE_ROOT}/impala/bin/impala-config.sh && ${MVN_CMD} -B -DskipTests ${FORTIFY_ARGS} -Dfortify.sca.buildId=impala-${impala_jar_version} ${FORTIFY_CLEAN_CMD}", impala-parent
    cmd_4 = bash -c " . ${SOURCE_ROOT}/impala/bin/impala-config.sh && ${MVN_CMD} -B -DskipTests ${FORTIFY_ARGS} -Dfortify.sca.buildId=impala-${impala_jar_version} ${FORTIFY_TRANSLATE_CMD}", impala-parent
    cmd_5 = bash -c " . ${SOURCE_ROOT}/impala/bin/impala-config.sh && ${MVN_CMD} -B -DskipTests ${FORTIFY_ARGS} -Dfortify.sca.buildId=impala-${impala_jar_version} ${FORTIFY_CLEAN_CMD}", query-event-hook-api
    cmd_6 = bash -c " . ${SOURCE_ROOT}/impala/bin/impala-config.sh && ${MVN_CMD} -B -DskipTests ${FORTIFY_ARGS} -Dfortify.sca.buildId=impala-${impala_jar_version} ${FORTIFY_TRANSLATE_CMD}", query-event-hook-api
    cmd_7 = bash -c " . ${SOURCE_ROOT}/impala/bin/impala-config.sh && ${MVN_CMD} -B -DskipTests ${FORTIFY_ARGS} -Dfortify.sca.buildId=impala-${impala_jar_version} ${FORTIFY_CLEAN_CMD}", shaded-deps
    cmd_8 = bash -c " . ${SOURCE_ROOT}/impala/bin/impala-config.sh && ${MVN_CMD} -B -DskipTests ${FORTIFY_ARGS} -Dfortify.sca.buildId=impala-${impala_jar_version} ${FORTIFY_TRANSLATE_CMD}", shaded-deps
    cmd_9 = bash -c " . ${SOURCE_ROOT}/impala/bin/impala-config.sh && ${MVN_CMD} -B -DskipTests ${FORTIFY_ARGS} -Dfortify.sca.buildId=impala-${impala_jar_version} ${FORTIFY_CLEAN_CMD}", ext-data-source
    cmd_10 = bash -c " . ${SOURCE_ROOT}/impala/bin/impala-config.sh && ${MVN_CMD} -B -DskipTests ${FORTIFY_ARGS} -Dfortify.sca.buildId=impala-${impala_jar_version} ${FORTIFY_TRANSLATE_CMD}", ext-data-source
    cmd_11 = bash -c " . ${SOURCE_ROOT}/impala/bin/impala-config.sh && ${MVN_CMD} -B -DskipTests ${FORTIFY_ARGS} -Dfortify.sca.buildId=impala-${impala_jar_version} ${FORTIFY_CLEAN_CMD}", common/yarn-extras
    cmd_12 = bash -c " . ${SOURCE_ROOT}/impala/bin/impala-config.sh && ${MVN_CMD} -B -DskipTests ${FORTIFY_ARGS} -Dfortify.sca.buildId=impala-${impala_jar_version} ${FORTIFY_TRANSLATE_CMD}", common/yarn-extras
    cmd_13 = bash -c " . ${SOURCE_ROOT}/impala/bin/impala-config.sh && ${MVN_CMD} -B -DskipTests ${FORTIFY_ARGS} -Dfortify.sca.buildId=impala-${impala_jar_version} ${FORTIFY_CLEAN_CMD}", fe
    cmd_14 = bash -c " . ${SOURCE_ROOT}/impala/bin/impala-config.sh && ${MVN_CMD} -B -DskipTests ${FORTIFY_ARGS} -Dfortify.sca.buildId=impala-${impala_jar_version} ${FORTIFY_TRANSLATE_CMD}", fe
    cmd_15 = sourceanalyzer -b impala-${impala_jar_version} ${FORTIFY_SCAN_MEMORY} -scan -f impala-${impala_jar_version}.fpr

[hue]
build_tool = maven
COMMON_BUILD_OPTS = "${MVN_CMD} -Drepoid=${NEXUS_DEPLOY_REPO_ID} -Drepourl=${NEXUS_REPO_URL} -Dreponame=${NEXUS_DEPLOY_REPO_ID} ${MAVEN_TEST_OPTS}"
deploy_cmd = ${COMMON_BUILD_OPTS} -DskipTests deploy
coverage_tool = cobertura
PYPI_MIRROR = "https://pypi.infra.cloudera.com/api/pypi/pypi-public/simple/"
PATH = ${PYTHON27_PATH}:${PATH}
DOCKER_REGISTRY = "docker-private.infra.cloudera.com/cdh"

    [[setversion_cmd]]
    cmd_1 = ${MVN_CMD} -f maven/pom.xml ${MVN_SET_VERSION_CMD} -DnewVersion=${hue_jar_version}
    cmd_2 = ${MVN_CMD} -f desktop/libs/librdbms/java/pom.xml versions:update-parent -DparentVersion=${hue_jar_version}

    [[artifacts]]
    artifact_1 = build/hue-${hue_jar_version}.tar.gz
    artifact_2 = desktop/desktop.db

    [[install_cmd]]
    cmd_0 = bash -c "REGISTRY=${DOCKER_REGISTRY} GBN=${STACKVERSION}-${BUILDNUMBER} bash ./tools/container/build.sh"
    cmd_1 = bash -c "mkdir -p build/release/prod/"
    cmd_2 = ${COMMON_BUILD_OPTS} -DskipTests -f maven/pom.xml install -Dmaven.javadoc.skip=true -DlocalRepositoryPath=build/release/prod/
    #	Python virtual environments are created by Makefile and relocatable.sh
    cmd_3 = bash -c "PYPI_MIRROR=${PYPI_MIRROR} make apps docs"
    cmd_4 = bash -c "PYPI_MIRROR=${PYPI_MIRROR} bash ./tools/relocatable.sh"
    cmd_5 = bash -c "PYPI_MIRROR=${PYPI_MIRROR} make prod"
    cmd_6 = bash -c "rm -rf build/release/prod/hue-*.tgz build/release/prod/hue-${hue_jar_version}"
    cmd_7 = bash -c "mv build/release/prod/hue-* build/release/prod/hue-${hue_jar_version}"
	cmd_8 = tar --use-compress-program pigz -C build/release/prod -cf "build/hue-${hue_jar_version}.tar.gz" "hue-${hue_jar_version}"

    [[xml-replace]]
        REPLACE_1 = parent/version, ${hue_jar_version}, desktop/libs/librdbms/java/pom.xml

    [[text-replace]]
        replace_1 = "__REGISTRY__", ${DOCKER_REGISTRY}, cloudera/docker_images.yaml, regex_replace

    [[docker_metadata_cmd]]
        cmd_1 = cloudera/docker_images.yaml, file

[kafka]
build_tool = gradle
COMMON_BUILD_OPTS = "{GRADLE_CMD} ${GRADLE_OPTS} "
deploy_cmd = ${COMMON_BUILD_OPTS} deploy
kafka_scala_version = 2.11
kafka_scala_version_compile = 2.11.12
package_count = 2

BUILD_KAFKA_SETVERSION_OPTS = "-Pversion=${kafka_jar_version}"
BUILD_KAFKA_OPTS = "${BUILD_KAFKA_SETVERSION_OPTS}"
BUILD_KAFKA_DOC_OPTS = "${BUILD_KAFKA_OPTS} docsJar"
BUILD_KAFKA_INSTALL_OPTS = "${BUILD_KAFKA_OPTS} clean jar"
BUILD_KAFKA_EXAMPLES_INSTALL_OPTS = "${BUILD_KAFKA_OPTS} examples:jar"
BUILD_KAFKA_DEPLOY_OPTS = "${BUILD_KAFKA_OPTS} releaseTarGz"
BUILD_KAFKA_TEST_OPTS = "${BUILD_KAFKA_OPTS} cleanTest testWithFlakyRetry -PrepoUrl=${NEXUS_PROXY_URL} -Pgbnurl=${GBN_MVN_REPO}"
BUILD_KAFKA_UPLOAD = "${BUILD_KAFKA_SETVERSION_OPTS} installAll"

    [[artifacts]]
        artifact_1 = core/build/distributions/kafka_${kafka_scala_version}-${kafka_jar_version}.tgz
        artifact_2 = kafka.tar.gz

    [[install_cmd]]
        cmd_0 = ${GRADLE4102_CMD}
        cmd_1 = ./gradlew ${BUILD_KAFKA_SETVERSION_OPTS}
        cmd_2 = ./gradlew ${BUILD_KAFKA_INSTALL_OPTS}
        cmd_3 = ./gradlew ${BUILD_KAFKA_DOC_OPTS}
        cmd_4 = ./gradlew ${BUILD_KAFKA_EXAMPLES_INSTALL_OPTS}
        cmd_6 = ./gradlew ${BUILD_KAFKA_DEPLOY_OPTS}
        cmd_7 = ./gradlew ${BUILD_KAFKA_UPLOAD}
        cmd_10 =  "tar -zcvf kafka.tar.gz core/build/docs NOTICE LICENSE"

   [[test_cmd]]
        cmd_1 = ./gradlew ${BUILD_KAFKA_TEST_OPTS}

   [[fortify_cmd]]
        # TO DO Replace gradlew PATH. For now hard coding the path.
        cmd_0 = sourceanalyzer -b kafka-${kafka_jar_version} -source "1.7" ${GRADLE4102_CMD}
        cmd_1 = sourceanalyzer -b kafka-${kafka_jar_version} -source "1.7" ${SOURCE_ROOT}/kafka/gradlew ${BUILD_KAFKA_SETVERSION_OPTS}
        cmd_2 = sourceanalyzer -b kafka-${kafka_jar_version} -source "1.7" ${SOURCE_ROOT}/kafka/gradlew ${BUILD_KAFKA_INSTALL_OPTS}
        cmd_3 = sourceanalyzer -b kafka-${kafka_jar_version} -source "1.7" ${SOURCE_ROOT}/kafka/gradlew ${BUILD_KAFKA_DOC_OPTS}
        cmd_4 = sourceanalyzer -b kafka-${kafka_jar_version} -source "1.7" ${SOURCE_ROOT}/kafka/gradlew ${BUILD_KAFKA_EXAMPLES_INSTALL_OPTS}
        cmd_6 = sourceanalyzer -b kafka-${kafka_jar_version} ${FORTIFY_SCAN_MEMORY} -scan -f kafka-${kafka_jar_version}.fpr

   [[text-replace]]
        REPLACE_1 = 'scalaVersion', ${kafka_scala_version_compile}, gradle.properties , key_value
        REPLACE_2 = 'version', ${kafka_jar_version}, gradle.properties , key_value
        REPLACE_3 = 'zookeeperVersion', ${zookeeper_jar_version}, gradle.properties , key_value
        REPLACE_4 = 'mavenUrl', ${NEXUS_PROXY_URL}, gradle.properties , key_value
        REPLACE_5 = 'gbnUrl', ${GBN_MVN_REPO}, gradle.properties , key_value

[knox]
build_tool = maven
COMMON_BUILD_OPTS = "${MVN_CMD}  -Ppackage,release,idbroker -DskipCheck=true -Dcheckstyle.skip=true -Drepo.id=${NEXUS_DEPLOY_REPO_ID} -Dinternal.maven.repo=${NEXUS_REPO_URL}"
setversion_cmd = ${MVN_CMD} ${MVN_SET_VERSION_CMD} -DnewVersion=${knox_jar_version}
deploy_cmd = ${COMMON_BUILD_OPTS}  deploy -DskipITs -DskipTests
package = 2
coverage_tool = cobertura

    [[artifacts]]
        artifact_1 = target/${knox_jar_version}/knox-${knox_jar_version}.tar.gz

    [[install_cmd]]
        cmd_1 = ${COMMON_BUILD_OPTS} install -DskipITs -DskipTests -Dmaven.javadoc.skip=true

    [[fortify_cmd]]
        cmd_1 = ${COMMON_BUILD_OPTS} install -DskipITs -DskipTests -Dmaven.javadoc.skip=true
        cmd_2 = ${COMMON_BUILD_OPTS} -DskipITs -DskipTests -Dmaven.javadoc.skip=true ${FORTIFY_ARGS} -Dfortify.sca.buildId=knox-${knox_jar_version} ${FORTIFY_CLEAN_CMD}
        cmd_3 = ${COMMON_BUILD_OPTS} -DskipITs -DskipTests -Dmaven.javadoc.skip=true ${FORTIFY_ARGS} -Dfortify.sca.buildId=knox-${knox_jar_version} ${FORTIFY_TRANSLATE_CMD}
        cmd_4 = sourceanalyzer -b knox-${knox_jar_version} ${FORTIFY_SCAN_MEMORY} -scan -f knox-${knox_jar_version}.fpr

    [[test_cmd]]
        cmd_1 = "${MVN_CMD} -DskipTests=false -Ppackage,release,idbroker install"

    [[test_coverage_cmd]]
        cmd_1 = "${MVN_CMD} -DskipTests=false -Ppackage,release,idbroker cobertura:cobertura"

    [[xml-replace]]
    REPLACE_1 = 'gateway.version', ${knox_jar_version}, pom.xml
    REPLACE_2 = 'hadoop.version', ${hadoop_jar_version}, pom.xml
    REPLACE_3 = 'zookeeper.version', ${zookeeper_jar_version}, pom.xml
    REPLACE_4 = 'gcs.version', ${gcs_jar_version}, gateway-cloud-bindings/pom.xml

[kudu]
build_tool = gradle
CSD_BUILD_DIR = "${SOURCE_ROOT}/kudu/build/packages/kudu-csd-mvn/"

    [[artifacts]]
        artifact_1 = kudu-${kudu_jar_version}.tar.gz
        artifact_2 = kudu-csd-${kudu_jar_version}.tar.gz

    [[install_cmd]]
        cmd_1 = bash -c "./cloudera/cdpd_install_cmd.sh"
        cmd_2 = bash -c "./cloudera/build_csd.sh ${CSD_BUILD_DIR}"

    [[fortify_cmd]]
        cmd_1 = bash -c "./cloudera/cdpd_install_cmd.sh"
        cmd_2 = sourceanalyzer -b kudu-${kudu_jar_version} -source "1.8" ${SOURCE_ROOT}/kudu/java/gradlew install -PskipSigning=true, java
        cmd_3 = sourceanalyzer -b kudu-${kudu_jar_version} -Xmx48G -scan -f ${kudu_jar_version}.fpr

    [[text-replace]]
        # These textual substitutions harmonize the versions of various Gradle
        # dependencies with those found in CDPD.
        REPLACE_1 = 'avro *: "[^:\$]+"', 'avro : "${avro_jar_version}"', java/gradle/dependencies.gradle , regex_replace
        REPLACE_2 = 'hadoop *: "[^:\$]+"', 'hadoop : "${hadoop_jar_version}"', java/gradle/dependencies.gradle , regex_replace
        REPLACE_3 = 'hive *: "[^:\$]+"', 'hive : "${hive_jar_version}"', java/gradle/dependencies.gradle , regex_replace
        REPLACE_4 = 'parquet *: "[^:\$]+"', 'parquet : "${parquet_jar_version}"', java/gradle/dependencies.gradle , regex_replace
        REPLACE_5 = 'slf4j *: "[^:\$]+"', 'slf4j : "1.7.25"', java/gradle/dependencies.gradle , regex_replace
        REPLACE_6 = 'spark *: "[^:\$]+"', 'spark : "${spark_jar_version}"', java/gradle/dependencies.gradle , regex_replace
        REPLACE_7 = 'flume *: "[^:\$]+"', 'flume : "${flume-ng_jar_version}"', java/gradle/dependencies.gradle , regex_replace
        # Use the private Cloudera repository.
        REPLACE_8 = 'repositoryUrl[^=]*=.*', "repositoryUrl=${NEXUS_PROXY_URL}", java/gradle.properties , regex_replace
        REPLACE_9 = 'gbnUrl[^=]*=.*', "gbnUrl=${GBN_MVN_REPO}", java/gradle.properties , regex_replace
        # Change the versions on the CSD artifacts to match the branch we're
        # building rather than e.g. 7.x.0.
        REPLACE_10 = 'cmVersion[^=]*=.*', "cmVersion=${STACKVERSION}", java/gradle.properties, regex_replace
        # Substitute in the version that's built into Kudu.
        REPLACE_11 = '.+', "${kudu_jar_version}", version.txt , regex_replace

[livy]
build_tool = maven
COMMON_BUILD_OPTS = "${MVN_CMD} ${MAVEN_TEST_OPTS} -Dtar -Pgpg -Pthriftserver -Dhadoop.version=${hadoop_jar_version} -Drepo.id=${NEXUS_DEPLOY_REPO_ID} -Dinternal.maven.repository=${NEXUS_REPO_URL} -DaltDeploymentRepository=${NEXUS_DEPLOY_REPO_ID}::default::${NEXUS_REPO_URL} -DdistMgmtStagingId=${NEXUS_DEPLOY_REPO_ID} -DdistMgmtStagingUrl=${NEXUS_REPO_URL}"
setversion_cmd = ${MVN_CMD} ${MVN_SET_VERSION_CMD} -DnewVersion=${livy_jar_version}
deploy_cmd = ${COMMON_BUILD_OPTS} deploy
coverage_tool = cobertura

    [[artifacts]]
        artifact_1 = assembly/target/livy-server-${livy_jar_version}.zip

    [[install_cmd]]
        cmd_1 = ${COMMON_BUILD_OPTS} install -Dmaven.javadoc.skip=true -DskipTests -Dskip -DskipITs

    [[fortify_cmd]]
        cmd_1 = ${COMMON_BUILD_OPTS} -Dmaven.javadoc.skip=true -DskipTests -Dskip -DskipITs install
        cmd_2 = ${COMMON_BUILD_OPTS} -Dmaven.javadoc.skip=true -DskipTests -Dskip -DskipITs ${FORTIFY_ARGS} -Dfortify.sca.buildId=livy-${livy_jar_version} ${FORTIFY_CLEAN_CMD}
        cmd_3 = ${COMMON_BUILD_OPTS} -Dmaven.javadoc.skip=true -DskipTests -Dskip -DskipITs ${FORTIFY_ARGS} -Dfortify.sca.buildId=livy-${livy_jar_version} ${FORTIFY_TRANSLATE_CMD}
        cmd_4 = sourceanalyzer -b livy-${livy_jar_version} ${FORTIFY_SCAN_MEMORY} -scan -f livy-${livy_jar_version}.fpr

    [[test_cmd]]
        cmd_1 = ${COMMON_BUILD_OPTS} verify -pl !coverage -pl !python-api

    [[test_coverage_cmd]]
        cmd_1 = ${COMMON_BUILD_OPTS} cobertura:cobertura -pl !python-api

    [[xml-replace]]
        REPLACE_1 = 'hadoop.version', ${hadoop_jar_version}, pom.xml
        REPLACE_2 = 'spark.version', ${spark_jar_version}, pom.xml
        REPLACE_3 = 'spark1.version', ${spark1_lkgb_jar_version}, pom.xml
        REPLACE_4 = 'spark.version', ${spark_jar_version}, repl/scala-2.11/pom.xml
        REPLACE_5 = 'spark.version', ${spark_jar_version}, scala-api/scala-2.11/pom.xml

[phoenix]
build_tool = maven
COMMON_BUILD_OPTS = "${MVN_CMD} -Drepoid=${NEXUS_DEPLOY_REPO_ID} -Dreponame=${NEXUS_DEPLOY_REPO_ID} -Drepourl=${NEXUS_REPO_URL} -Drepo.maven.org=${NEXUS_PROXY_URL}"
setversion_cmd = ${MVN_CMD} ${MVN_SET_VERSION_CMD} -DnewVersion=${phoenix_jar_version}
deploy_cmd = ${COMMON_BUILD_OPTS} clean package deploy -Pgpg -DskipITs -DskipTests

    [[artifacts]]
        artifact_1 = phoenix-assembly/target/phoenix-${phoenix_jar_version}.tar.gz

    [[install_cmd]]
        cmd_1 = ${COMMON_BUILD_OPTS} clean install -DskipITs -DskipTests -Dmaven.javadoc.skip=true

    [[fortify_cmd]]
        cmd_1 = ${COMMON_BUILD_OPTS} -DskipITs -DskipTests -Dmaven.javadoc.skip=true clean install
        cmd_2 = ${COMMON_BUILD_OPTS} -DskipITs -DskipTests -Dmaven.javadoc.skip=true ${FORTIFY_ARGS} -Dfortify.sca.buildId=phoenix-${phoenix_jar_version} ${FORTIFY_CLEAN_CMD}
        cmd_3 = ${COMMON_BUILD_OPTS} -DskipITs -DskipTests -Dmaven.javadoc.skip=true ${FORTIFY_ARGS} -Dfortify.sca.buildId=phoenix-${phoenix_jar_version} ${FORTIFY_TRANSLATE_CMD}
    	cmd_4 = sourceanalyzer -b phoenix-${phoenix_jar_version} ${FORTIFY_SCAN_MEMORY} -scan -f phoenix-${phoenix_jar_version}.fpr

# BUG-107144 we seem to inherit a value for HADOOP_CONF_DIR somewhere in the environment
#   which has been observed to directly affect the minicluster for the phoenix-hive tests when set.
    [[test_cmd]]
        cmd_1 = HADOOP_CONF_DIR="" ${COMMON_BUILD_OPTS} ${MAVEN_TEST_OPTS} -DfailIfNoTests=false -DreuseForks=false verify

    [[test_coverage_cmd]]
        cmd_1 = ${COMMON_BUILD_OPTS} ${MAVEN_TEST_OPTS} -DfailIfNoTests=false -DreuseForks=false cobertura:cobertura

    [[xml-replace]]
        REPLACE_2 = 'hadoop-two.version', ${hadoop_jar_version}, pom.xml
        REPLACE_3 = 'hadoop.version', ${hadoop_jar_version}, pom.xml
        REPLACE_4 = 'hbase.version', ${hbase_jar_version}, pom.xml
        REPLACE_5 = 'pig.version', ${pig_jar_version}, pom.xml
        REPLACE_7 = 'avatica.version', ${avatica_jar_version}, pom.xml
        REPLACE_8 = 'calcite.version', ${calcite_jar_version}, pom.xml
        REPLACE_9 = 'hive.version', ${hive_jar_version}, pom.xml
        REPLACE_10 = 'spark.version', ${spark_jar_version}, pom.xml
        REPLACE_11 = 'tez.version', ${tez_jar_version}, pom.xml
        REPLACE_12 = 'kafka.version', ${kafka_jar_version}, pom.xml
        #For Zookeeper compatibility
        REPLACE_13 = 'slf4j.version' , '1.7.25', pom.xml
        REPLACE_14 = 'log4j.version' , '1.2.17', pom.xml

[pig]
build_tool = ant
COMMON_BUILD_OPTS = "${ANT_CMD} -Djavac.version=1.7 -Dversion=${pig_jar_version} -Dzookeeper.version=${zookeeper_jar_version} -Dhive.version=${hive_jar_version} -Dtez.version=${tez_jar_version} -Daccumulo15.version=${accumulo_jar_version} -Dhbase2.version=${hbase_jar_version} -Dhadoop-common.version=${hadoop_jar_version} -Dhadoop-hdfs.version=${hadoop_jar_version} -Dhadoop-mapreduce.version=${hadoop_jar_version} -Dtest.junit.output.format=xml -Dforrest.home=${FORREST_HOME} -Dhadoopversion=23 -Dorc.version=${orc_jar_version} -Dmvnrepo=${NEXUS_PROXY_URL} -Drepo.apache.snapshots=${NEXUS_PROXY_URL} -Drepo.jboss.org=${GBN_MVN_REPO}"
deploy_cmd = ${COMMON_BUILD_OPTS} -f nexus-build.xml -Dstaging_repo_id=${NEXUS_DEPLOY_REPO_ID} -Dasfstagingrepo=${NEXUS_REPO_URL} stage
BUILD_PIG_COMPILE_JAR="${COMMON_BUILD_OPTS} very-clean jar pigunit-jar smoketests-jar compile-test"

    [[artifacts]]
        artifact_1 = build/pig-${pig_jar_version}.tar.gz
        artifact_2 = pig.tar.gz
        artifact_3 = build/pig-${pig_jar_version}-smoketests.jar

    [[install_cmd]]
        cmd_1 = ${COMMON_BUILD_OPTS} mvn-install
        cmd_2 = ${COMMON_BUILD_OPTS} very-clean jar pigunit-jar smoketests-jar compile-test
        cmd_3 = ${COMMON_BUILD_OPTS} -buildfile contrib/piggybank/java/build.xml clean jar
        cmd_4 = ${COMMON_BUILD_OPTS} -Dant-task.version=2.1.3 tar
        cmd_5 = "tar -cz --exclude-vcs --transform 's,^build\/tar\/pig-${pig_jar_version}\/,pig/,' -f pig.tar.gz build/tar/pig-${pig_jar_version}"

    [[fortify_cmd]]
        cmd_1 = ${COMMON_BUILD_OPTS} mvn-install
        cmd_2 = ${COMMON_BUILD_OPTS} very-clean jar pigunit-jar smoketests-jar compile-test
        cmd_3 = ${COMMON_BUILD_OPTS} -buildfile contrib/piggybank/java/build.xml clean jar
        cmd_4 = ${COMMON_BUILD_OPTS} -Dant-task.version=2.1.3 tar
        cmd_5 = "tar -cz --exclude-vcs --transform 's,^build\/tar\/pig-${pig_jar_version}\/,pig/,' -f pig.tar.gz build/tar/pig-${pig_jar_version}"
        cmd_6 = ${COMMON_BUILD_OPTS} -Dbuild.compiler=com.fortify.dev.ant.SCACompiler -Dsourceanalyzer.buildid=${pig_jar_version} -lib ${FORTIFY_SCA_HOME}/Core/lib/sourceanalyzer.jar -Dsourceanalyzer.maxHeap=32G
        # cmd_7 = sourceanalyzer -b ${pig_jar_version} -Xmx48G -verbose -cp "./jars/*.jar" -source "1.7" "**/*.jsp" "**/*.xml" "**/*.js" "**/*.properties" "**/*.java"
        cmd_7 = sourceanalyzer -b ${pig_jar_version} -Xmx48G -verbose -cp "./jars/*.jar" -source "1.7" "src/**/*.java"
        cmd_8 = sourceanalyzer -b ${pig_jar_version} -Xmx48G -scan -f ${pig_jar_version}.fpr

    [[test_cmd]]
        cmd_1 = ${COMMON_BUILD_OPTS} -Dant-task.version=2.1.3 test-core-mrtez


[pydruid]
build_tool = python
no_package = True

    [[artifacts]]
           artifact_1 = dist/pydruid-0.4.2-py2.py3-none-any.whl

    [[install_cmd]]
           cmd_1 = ${BASE_DIR}/buildvenv/bin/python setup.py bdist_wheel

[parquet]
build_tool = maven
setversion_cmd = ${MVN_CMD} ${MVN_SET_VERSION_CMD} -DnewVersion=${parquet_jar_version}
deploy_cmd = "${MVN_CMD} deploy -Drepo.id=${NEXUS_DEPLOY_REPO_ID} -Dinternal.maven.repo=${NEXUS_REPO_URL} -DskipITs -DskipTests"
PATH=${PROTOBUF351_HOME}/bin:${PATH}

    [[artifacts]]
        artifact_1 = parquet-${parquet_jar_version}.tar.gz

    [[install_cmd]]
        cmd_1 = ${MVN_CMD} -B clean install -DskipTests -Plocal
        cmd_2 = mkdir ../parquet-${parquet_jar_version}
        cmd_3 = cp -rpf . ../parquet-${parquet_jar_version}
        cmd_4 = tar --exclude-vcs -czf parquet-${parquet_jar_version}.tar.gz ../parquet-${parquet_jar_version}
        cmd_5 = rm -rf ../parquet-${parquet_jar_version}

    [[fortify_cmd]]
        cmd_1 = ${MVN_CMD} -DskipTests clean install
        cmd_2 = ${MVN_CMD} -DskipTests ${FORTIFY_ARGS} -Dfortify.sca.buildId=parquet-${parquet_jar_version} ${FORTIFY_CLEAN_CMD}
        cmd_3 = ${MVN_CMD} -DskipTests ${FORTIFY_ARGS} -Dfortify.sca.buildId=parquet-${parquet_jar_version} ${FORTIFY_TRANSLATE_CMD}
        cmd_4 = sourceanalyzer -b parquet-${parquet_jar_version} ${FORTIFY_SCAN_MEMORY} -scan -f parquet-${parquet_jar_version}.fpr

    [[xml-replace]]
        REPLACE_1 = 'hadoop.version', ${hadoop_jar_version}, pom.xml
        REPLACE_2 = 'pig.version', ${pig_lkgb_jar_version}, pom.xml
        REPLACE_3 = 'avro.version', ${avro_jar_version}, pom.xml
        REPLACE_4 = 'thrift.version', 0.9.3, pom.xml

[ratis]
build_tool = maven
setversion_cmd = ${MVN_CMD} ${MVN_SET_VERSION_CMD} -DnewVersion=${ratis_jar_version}
deploy_cmd = "${MVN_CMD} deploy -Drepo.id=${NEXUS_DEPLOY_REPO_ID} -Dinternal.maven.repo=${NEXUS_REPO_URL} -DskipITs -DskipTests"
PATH=${PROTOBUF351_HOME}/bin:${PATH}
JAVA_HOME=/grid/0/jenkins/tools/jdk8/jdk1.8.0_171
no_package = True

    [[install_cmd]]
        cmd_1 = ${MVN_CMD} -B clean install -DskipTests

    [[fortify_cmd]]
        cmd_1 = ${MVN_CMD} -DskipTests clean install
        cmd_2 = ${MVN_CMD} -DskipTests ${FORTIFY_ARGS} -Dfortify.sca.buildId=ratis-${ratis_jar_version} ${FORTIFY_CLEAN_CMD}
        cmd_3 = ${MVN_CMD} -DskipTests ${FORTIFY_ARGS} -Dfortify.sca.buildId=ratis-${ratis_jar_version} ${FORTIFY_TRANSLATE_CMD}
        cmd_4 = sourceanalyzer -b ratis-${ratis_jar_version} ${FORTIFY_SCAN_MEMORY} -scan -f ratis-${ratis_jar_version}.fpr

[ranger]
build_tool = maven
COMMON_BUILD_OPTS = "${MVN333_CMD} -DskipCheck=true -Dcheckstyle.skip=true -DdistMgmtStagingId=${NEXUS_DEPLOY_REPO_ID} -DdistMgmtStagingUrl=${NEXUS_REPO_URL}"
setversion_cmd = ${MVN333_CMD} ${MVN_SET_VERSION_CMD} -DnewVersion=${ranger_jar_version}
deploy_cmd = ${COMMON_BUILD_OPTS} package assembly:assembly deploy -Pgpg -DskipITs -DskipTests -Dmaven.javadoc.skip=true

    [[artifacts]]
        artifact_1 = target/ranger-${ranger_jar_version}-admin.tar.gz
        artifact_2 = target/ranger-${ranger_jar_version}-hbase-plugin.tar.gz
        artifact_3 = target/ranger-${ranger_jar_version}-hdfs-plugin.tar.gz
        artifact_4 = target/ranger-${ranger_jar_version}-hive-plugin.tar.gz
        artifact_5 = target/ranger-${ranger_jar_version}-kafka-plugin.tar.gz
        artifact_6 = target/ranger-${ranger_jar_version}-kms.tar.gz
        artifact_7 = target/ranger-${ranger_jar_version}-knox-plugin.tar.gz
        artifact_8 = target/ranger-${ranger_jar_version}-migration-util.tar.gz
        artifact_9 = target/ranger-${ranger_jar_version}-ranger-tools.tar.gz
        artifact_10 = target/ranger-${ranger_jar_version}-solr-plugin.tar.gz
        artifact_12 = target/ranger-${ranger_jar_version}-tagsync.tar.gz
        artifact_13 = target/ranger-${ranger_jar_version}-usersync.tar.gz
        artifact_14 = target/ranger-${ranger_jar_version}-yarn-plugin.tar.gz
        artifact_15 = target/ranger-${ranger_jar_version}-src.tar.gz
        artifact_16 = target/ranger-${ranger_jar_version}-atlas-plugin.tar.gz
        artifact_17 = target/ranger-${ranger_jar_version}-solr_audit_conf.zip
        artifact_18 = target/ranger-${ranger_jar_version}-ozone-plugin.tar.gz

    [[install_cmd]]
        cmd_1 = ${COMMON_BUILD_OPTS} package assembly:assembly install -Pgpg -DskipITs -DskipTests -Dmaven.javadoc.skip=true

    [[fortify_cmd]]
        cmd_1 = ${COMMON_BUILD_OPTS} package assembly:assembly install -Pgpg -DskipITs -DskipTests -Dmaven.javadoc.skip=true
        cmd_2 = ${COMMON_BUILD_OPTS} -Pgpg -DskipTests -DskipITs -Dmaven.javadoc.skip=true ${FORTIFY_ARGS} -Dfortify.sca.buildId=ranger-${ranger_jar_version} ${FORTIFY_CLEAN_CMD}
        cmd_3 = ${COMMON_BUILD_OPTS} -Pgpg -DskipTests -DskipITs -Dmaven.javadoc.skip=true ${FORTIFY_ARGS} -Dfortify.sca.buildId=ranger-${ranger_jar_version} ${FORTIFY_TRANSLATE_CMD}
        cmd_4 = sourceanalyzer -b ranger-${ranger_jar_version} ${FORTIFY_SCAN_MEMORY} -scan -f ranger-${ranger_jar_version}.fpr

    [[test_cmd]]
        cmd_1 = ${COMMON_BUILD_OPTS} test -DfailIfNoTests=false ${MAVEN_TEST_OPTS}

    [[test_coverage_cmd]]
        cmd_1 = ${COMMON_BUILD_OPTS} cobertura:cobertura -DfailIfNoTests=false ${MAVEN_TEST_OPTS}

    [[xml-replace]]
        REPLACE_1 = 'hadoop.version', ${hadoop_jar_version}, pom.xml
        REPLACE_2 = 'hadoop-auth.version', ${hadoop_jar_version}, pom.xml
        REPLACE_3 = 'hadoop-common.version', ${hadoop_jar_version}, pom.xml
        REPLACE_4 = 'hbase.version', ${hbase_jar_version}, pom.xml
        REPLACE_5 = 'hive.version', ${hive_jar_version}, pom.xml
        REPLACE_6 = 'kafka.version', ${kafka_jar_version}, pom.xml
        REPLACE_7 = 'zookeeper.version', ${zookeeper_jar_version}, pom.xml
        REPLACE_8 = 'tez.version', ${tez_jar_version}, pom.xml
        REPLACE_9 = 'calcite.version', ${calcite_jar_version}, pom.xml
        REPLACE_10 = 'knox.gateway.version', ${knox_jar_version}, pom.xml
        REPLACE_12 = 'atlas.version', ${atlas_jar_version}, pom.xml
	REPLACE_13 = 'avro.version' , ${avro_jar_version} , pom.xml

[search]
build_tool = maven
setversion_cmd = ${MVN_CMD} ${MVN_SET_VERSION_CMD} -DnewVersion=${search_jar_version}

    [[artifacts]]
        artifact_1 = search-dist/target/cloudera-search-${search_jar_version}-search-dist.tar.gz

    [[install_cmd]]
        cmd_1 = ${MVN_CMD} clean install -DskipTests

    [[fortify_cmd]]
        cmd_1 = ${MVN_CMD} -DskipTests clean install
        cmd_2 = ${MVN_CMD} -DskipTests ${FORTIFY_ARGS} -Dfortify.sca.buildId=search-${search_jar_version} ${FORTIFY_CLEAN_CMD}
        cmd_3 = ${MVN_CMD} -DskipTests ${FORTIFY_ARGS} -Dfortify.sca.buildId=search-${search_jar_version} ${FORTIFY_TRANSLATE_CMD}
        cmd_4 = sourceanalyzer -b search-${search_jar_version} ${FORTIFY_SCAN_MEMORY} -scan -f search-${search_jar_version}.fpr

    [[xml-replace]]
        REPLACE_1  = 'avro.version', ${avro_jar_version}, pom.xml
        REPLACE_2  = 'httpclient.version', '4.5.3', pom.xml
        REPLACE_3  = 'httpcomponents.core.version', '4.4.6', pom.xml
        REPLACE_5  = 'tika.version', '1.18', pom.xml
        REPLACE_6  = 'slf4j.version', '1.7.25', pom.xml
        REPLACE_7  = 'surefire.version', '2.20', pom.xml
        REPLACE_8  = 'crunch.version', ${crunch_jar_version}, pom.xml
        REPLACE_9  = 'hadoop.version', ${hadoop_jar_version}, search-mr/pom.xml
        REPLACE_10 = 'jackson.version', '2.10.0', pom.xml
        REPLACE_11 = 'commons-lang3.version', '3.7', search-mr/pom.xml
        REPLACE_12 = 'hadoop.version', ${hadoop_jar_version}, search-crunch/pom.xml
        REPLACE_14 = 'spark.version', ${spark_jar_version}, search-crunch/pom.xml
        REPLACE_15 = 'parquet.version', ${parquet_jar_version}, search-crunch/pom.xml
        REPLACE_16 = 'scala.base.version', '2.11',  search-crunch/pom.xml
        REPLACE_18 = 'snappy.version', '1.1.4', search-crunch/pom.xml
        REPLACE_19 = 'solr.version', ${solr_jar_version}, pom.xml,
        REPLACE_20 = 'solr.expected.version', ${solr_jar_version}, pom.xml
        REPLACE_21  = 'parquet.version', ${parquet_jar_version}, pom.xml
        REPLACE_22  = 'hadoop.version', ${hadoop_jar_version}, pom.xml
        REPLACE_23  = 'hive.version', ${hive_jar_version}, pom.xml
        REPLACE_24 = 'jetty.version', '9.3.27.v20190418', pom.xml

    [[text-replace]]
        REPLACE_1 = '\$\{cdh.zookeeper.version\}', ${zookeeper_jar_version}, pom.xml, regex_replace

[solr]
COMPONENT = solr
build_tool = ant
setversion_cmd = ${MVN_CMD} ${MVN_SET_VERSION_CMD} -DnewVersion=${solr_jar_version} -f cloudera/solr-upgrade/pom.xml
IVY_MIRROR_PROP = ${NEXUS_PROXY_URL}
M2_REPO_SUFFIX = ""
BUILD_OPTS="-Dversion=${solr_jar_version} -Dslf4j.binding=slf4j-log4j12 -Dexclude.from.war=nothing -Divy.home=${HOME}/.ivy2 -Drepo.maven.org=${IVY_MIRROR_PROP} -Divy_bootstrap_url1=${IVY_MIRROR_PROP} -Divy_install_path=${HOME}/tools/ant/latest/lib -lib ${HOME}/tools/ant/latest/lib -Dreactor.repo=file://${HOME}/.m2/repository${M2_REPO_SUFFIX} -Dcauldron-gbn=${IVY_MIRROR_PROP}"

        [[artifacts]]
          cmd_1 = build/solr-${solr_jar_version}.tar.gz

        [[install_cmd]]
          cmd_1 = ant ${BUILD_OPTS} ivy-bootstrap
          cmd_2 = mkdir -p "test-framework/lib"
          cmd_3 = ant ${BUILD_OPTS} clean
          cmd_4 = ant ${BUILD_OPTS} package-local-src-tgz , solr
          cmd_5 = ant ${BUILD_OPTS} create-package , solr
          cmd_6 = ant ${BUILD_OPTS} -Dcontrib-crawl.exclude=contrib/depends-sentry-libs/build.xml -Dmaven-deps.exclude="**/depends-sentry-libs/**" generate-maven-artifacts
          cmd_7 = rm -rf build
          cmd_8 = mkdir build
          cmd_9 = tar --use-compress-program pigz -C build -xf ${COMPONENT}/build/${COMPONENT}-${solr_jar_version}-src.tgz
          cmd_10 = tar --use-compress-program pigz -C build/${COMPONENT}-${solr_jar_version} --strip-components=1 -xf ${COMPONENT}/package/${COMPONENT}-${solr_jar_version}.tgz
          cmd_11 = cp -rf cloudera build/${COMPONENT}-${solr_jar_version}
          cmd_12 = cp -rf cdh.build.properties build/${COMPONENT}-${solr_jar_version}
          cmd_13 = tar --use-compress-program pigz -cf ${COMPONENT}-${solr_jar_version}.tar.gz ${COMPONENT}-${solr_jar_version} , build


        [[text-replace]]
          cmd_1 = 'cdh.solr.version' , ${solr_jar_version} , cloudera/templates/cdh.build.properties , key_value
          cmd_2 = 'hadoop.version' , ${hadoop_jar_version} , cloudera/templates/cdh.build.properties , key_value
          cmd_3 = 'avro.version' , ${avro_jar_version} , cloudera/templates/cdh.build.properties , key_value
          cmd_4 = 'zookeeper.version' , ${zookeeper_jar_version} , cloudera/templates/cdh.build.properties , key_value
          cmd_5 = 'releases.cloudera.com' , ${GBN_MVN_REPO} , cloudera/templates/cdh.build.properties , key_value
          cmd_6 = 'snapshots.cloudera.com', ${IVY_MIRROR_PROP} , cloudera/templates/cdh.build.properties , key_value
          cmd_7 = 'reactor.repo', ${IVY_MIRROR_PROP} , cloudera/templates/cdh.build.properties , key_value
          cmd_8 = 'org.eclipse.jetty.version', '9.3.27.v20190418', cloudera/templates/cdh.build.properties , key_value
          cmd_9 = 'property name="cauldron-gbn" value=".*" override', 'property name="cauldron-gbn" value="${GBN_MVN_REPO}" override', lucene/default-nested-ivy-settings.xml , regex_replace
          cmd_10 = 'property name="cauldron.cloudera.com" value=".*" override', 'property name="cauldron.cloudera.com" value="${GBN_MVN_REPO}" override', lucene/default-nested-ivy-settings.xml , regex_replace
          cmd_11 = 'property name="reactor.repo" value=".*" override', 'property name="reactor.repo" value="${IVY_MIRROR_PROP}" override', lucene/default-nested-ivy-settings.xml , regex_replace
          cmd_12 = 'property name="snapshots.cloudera.com" value=".*" override', 'property name="snapshots.cloudera.com" value="${IVY_MIRROR_PROP}" override', lucene/default-nested-ivy-settings.xml , regex_replace
          cmd_13 = 'property name="releases.cloudera.com" value=".*" override', 'property name="releases.cloudera.com" value="${GBN_MVN_REPO}" override', lucene/default-nested-ivy-settings.xml , regex_replace
          cmd_14 = 'ibiblio name="sonatype-releases" root=".*" m2compatible', 'ibiblio name="sonatype-releases" root="${IVY_MIRROR_PROP}" m2compatible', lucene/default-nested-ivy-settings.xml , regex_replace
          cmd_15 = 'ibiblio name="maven.restlet.org" root=".*" m2compatible', 'ibiblio name="maven.restlet.org" root="${IVY_MIRROR_PROP}" m2compatible', lucene/default-nested-ivy-settings.xml , regex_replace
          cmd_16 = 'ibiblio name="cloudera" root=".*" m2compatible', 'ibiblio name="cloudera" root="${IVY_MIRROR_PROP}" m2compatible', lucene/default-nested-ivy-settings.xml , regex_replace
          cmd_17 = 'ibiblio name="releases.cloudera.com" root=".*" m2compatible', 'ibiblio name="releases.cloudera.com" root="${GBN_MVN_REPO}" m2compatible', lucene/default-nested-ivy-settings.xml , regex_replace
          cmd_18 = 'ibiblio name="working-chinese-mirror" root=".*" m2compatible', 'ibiblio name="working-chinese-mirror" root="${IVY_MIRROR_PROP}" m2compatible', lucene/default-nested-ivy-settings.xml , regex_replace
          cmd_19 = 'property name="ivy_bootstrap_url1" value=".*"', 'property name="ivy_bootstrap_url1" value="${IVY_MIRROR_PROP}"', lucene/common-build.xml , regex_replace
          cmd_20 = 'property name="ivy_bootstrap_url2" value=".*"', 'property name="ivy_bootstrap_url2" value="${IVY_MIRROR_PROP}"', lucene/common-build.xml , regex_replace

    [[xml-replace]]
        REPLACE_1 = 'url', ${IVY_MIRROR_PROP}, cloudera/solr-upgrade/settings.xml
        REPLACE_2 = 'repository.root', ${GBN_MVN_REPO} , cloudera/solr-upgrade/settings.xml
        REPLACE_3 = url, ${IVY_MIRROR_PROP}, ./settings/mirrors/mirror/[id='cauldron-repo']/url, cloudera/solr-upgrade/settings.xml
        REPLACE_4 = url, ${IVY_MIRROR_PROP}, ./settings/mirrors/mirror/[id='repo1-cache']/url, cloudera/solr-upgrade/settings.xml
        REPLACE_5 = url, ${GBN_MVN_REPO}, ./settings/profiles/profile/repositories/repository/[id='cdh.repo']/url, cloudera/solr-upgrade/settings.xml
        REPLACE_6 = url, ${IVY_MIRROR_PROP}, ./settings/profiles/profile/repositories/repository/[id='external']/url, cloudera/solr-upgrade/settings.xml
        REPLACE_7 = url, ${GBN_MVN_REPO}, ./settings/profiles/profile/pluginRepositories/pluginRepository/[id='cdh.repo']/url, cloudera/solr-upgrade/settings.xml
        REPLACE_8 = url, ${IVY_MIRROR_PROP}, ./settings/profiles/profile/pluginRepositories/pluginRepository/[id='external']/url, cloudera/solr-upgrade/settings.xml
        REPLACE_9 = 'solr.version', ${solr_jar_version}, cloudera/solr-upgrade/pom.xml

    [[test_cmd]]
        cmd_1 = ant ${BUILD_OPTS} ivy-bootstrap resolve
        cmd_2 = ant ${BUILD_OPTS} clean test -Dtests.badapples=false -Dtests.jvms.override=2

[spark]
build_tool = maven
BUILD_SPARK_SETVERSION_OPTS = ${MVN_CMD} ${MVN_SET_VERSION_CMD} -DnewVersion=${spark_jar_version} -DgenerateBackupPoms=false
COMMON_BUILD_OPTS = "-Dcdpd.build=true -Phive-thriftserver -Psparkr -Drepo.id=${NEXUS_DEPLOY_REPO_ID} -Dinternal.maven.repo=${NEXUS_REPO_URL} -Dsurefire.timeout=9600 ${MAVEN_TEST_OPTS} -Dskip"
setversion_cmd = ${MVN_CMD} ${MVN_SET_VERSION_CMD} -DnewVersion=${spark_jar_version}
deploy_cmd = ${COMMON_BUILD_OPTS} package deploy -DskipTests -Dskip
package_count = 5
coverage_tool = cobertura

   [[PASSTHROUGH_ENV]]
        HADOOP_VERSION=${hadoop_jar_version}
        SPARK_VERSION=${spark_jar_version}
    [[artifacts]]
        artifact_1 = spark-${spark_jar_version}-bin-${hadoop_jar_version}.tgz

    [[install_cmd]]
        cmd_1 = dev/make-distribution.sh --tgz ${COMMON_BUILD_OPTS} -Dskip=true

    [[fortify_cmd]]
        cmd_1 = dev/make-distribution.sh --tgz ${COMMON_BUILD_OPTS} -Dskip=true
        cmd_2 = build/mvn ${COMMON_BUILD_OPTS} -DskipTests -Dmaven.javadoc.skip=true ${FORTIFY_ARGS} -Dfortify.sca.buildId=spark-${spark_jar_version} ${FORTIFY_CLEAN_CMD}
        cmd_3 = build/mvn ${COMMON_BUILD_OPTS} -DskipTests -Dmaven.javadoc.skip=true ${FORTIFY_ARGS} -Dfortify.sca.buildId=spark-${spark_jar_version} ${FORTIFY_TRANSLATE_CMD}
        cmd_4 = sourceanalyzer -b spark-${spark_jar_version} ${FORTIFY_SCAN_MEMORY} -scan -f spark-${spark_jar_version}.fpr

    [[test_cmd]]
        cmd_1 = ${MVN_CMD} ${COMMON_BUILD_OPTS} -fae test

    [[test_coverage_cmd]]
        cmd_1 = ${MVN_CMD} ${COMMON_BUILD_OPTS} -fae cobertura:cobertura

    [[xml-replace]]
        REPLACE_2 = 'hadoop.version', ${hadoop_jar_version}, pom.xml
        REPLACE_3 = 'hbase.version', ${hbase_jar_version}, pom.xml
        REPLACE_4 = 'protobuf.version', ${protobuf_version}, pom.xml
        REPLACE_5 = 'zookeeper.version', ${zookeeper_jar_version}, pom.xml
        REPLACE_6 = 'kafka.version', ${kafka_jar_version}, pom.xml
        REPLACE_7 = 'hive.version', ${spark_hive2_jar_version}, pom.xml
        REPLACE_8 = 'parquet.version', ${parquet_jar_version}, pom.xml
        REPLACE_9 = 'orc.version', ${orc_jar_version}, pom.xml


[spark_hive2]
build_tool = maven
setversion_cmd = ${MVN_CMD} ${MVN_SET_VERSION_CMD} -DnewVersion=${spark_hive2_jar_version}

    [[install_cmd]]
        cmd_1 = "${MVN_CMD} clean install -Dhadoop.mr.rev=23 -DskipSparkTests -Drepo.maven.org=${NEXUS_PROXY_URL} -Dtest.junit.output.format=xml -Dmvn.hadoop.profile=hadoop23 -Drepoid=${NEXUS_DEPLOY_REPO_ID} -Drepourl=${NEXUS_REPO_URL} -Dmvnrepo=${NEXUS_PROXY_URL} -DskipTests -Phadoop-2,dist,sources -Dmaven.javadoc.skip=true -Dgpg.skip"

    [[xml-replace]]
        #REPLACE_1 = 'hadoop-23.version', ${hadoop_jar_version}, pom.xml
        REPLACE_2 = 'hbase.version', ${hbase_jar_version}, pom.xml
        REPLACE_3 = 'protobuf.version', ${protobuf_version}, pom.xml
        REPLACE_4 = 'zookeeper.version', ${zookeeper_jar_version}, pom.xml
        #REPLACE_5 = 'kafka.version', ${kafka_jar_version}, pom.xml
        REPLACE_6 = 'accumulo.version', ${accumulo_jar_version}, pom.xml
        REPLACE_7 = 'hive3.version', ${hive_jar_version}, standalone-metastore/pom.xml

[sqoop]
build_tool = ant
COMMON_BUILD_OPTS = "${ANT_CMD} -Dmvn.version=2.1.3 -Dorc.version=${orc_jar_version} -Dhadoop.version=${hadoop_jar_version} -Dhbase.version=${hbase_jar_version} -Dhcatalog.version=${hive_jar_version} -Dzookeeper.version=${zookeeper_jar_version} -Daccumulo.version=${accumulo_jar_version} -Dversion=${sqoop_jar_version} -Dhadoop.version.full=${hadoop_jar_version} -Dprev.git.hash=HEAD -Dtest.junit.output.format=xml -Dslf4j.version=1.6.1 -Dmvn.repo=${NEXUS_DEPLOY_REPO_ID} -Dmvn.repo.id=${NEXUS_DEPLOY_REPO_ID} -Dmvn.deploy.url=${GBN_MVN_REPO} -Drepo.maven.org=${GBN_MVN_REPO} -Dsnapshot.apache.org=${NEXUS_PROXY_URL} -Dstaging.cloudera.com=${NEXUS_PROXY_URL} -Dreleases.cloudera.com=${NEXUS_PROXY_URL}"
deploy_cmd = ${COMMON_BUILD_OPTS} clean mvn-install mvn-deploy tar
package_count = 3
coverage_tool = cobertura

    [[artifacts]]
        artifact_1 = build/sqoop-${sqoop_jar_version}.bin__hadoop-${hadoop_jar_version}.tar.gz

    [[install_cmd]]
        cmd_1 = ${COMMON_BUILD_OPTS} clean mvn-install tar -Dmaven.javadoc.skip=true

    [[fortify_cmd]]
        cmd_1 = sourceanalyzer -b sqoop-${sqoop_jar_version} ${FORTIFY_SCAN_MEMORY} -verbose ${COMMON_BUILD_OPTS} clean mvn-install tar -Dmaven.javadoc.skip=true
        cmd_2 = sourceanalyzer -b sqoop-${sqoop_jar_version} ${FORTIFY_SCAN_MEMORY} -scan -f sqoop-${sqoop_jar_version}.fpr

    [[test_cmd]]
        cmd_1 = ${COMMON_BUILD_OPTS} clean test tar -Dmvn.version=2.1.3

    [[test_coverage_cmd]]
        cmd_1 = ${COMMON_BUILD_OPTS} clean cobertura:cobertura tar -Dmvn.version=2.1.3

[spark_atlas_connector]
build_tool = maven
setversion_cmd = ${MVN_CMD} ${MVN_SET_VERSION_CMD} -DnewVersion=${spark_atlas_connector_jar_version}

    [[artifacts]]
        artifact_1 = spark-atlas-connector-assembly/target/spark-atlas-connector-assembly-${spark_atlas_connector_jar_version}.jar

    [[install_cmd]]
        cmd_1 = ${MVN_CMD} clean install -DskipTests

    [[xml-replace]]
        REPLACE_1 = 'spark.version', ${spark_jar_version}, pom.xml
        REPLACE_2 = 'atlas.version', ${atlas_jar_version}, pom.xml
        REPLACE_3 = 'kafka.version', ${kafka_jar_version}, pom.xml

[spark_schema_registry]
build_tool = maven
setversion_cmd = ${MVN_CMD} ${MVN_SET_VERSION_CMD} -DnewVersion=${spark_schema_registry_jar_version}

    [[artifacts]]
        artifact_1 = assembly/target/hortonworks-spark-schema-registry-${spark_schema_registry_jar_version}.tar.gz

    [[install_cmd]]
        cmd_1 = ${MVN_CMD} clean install -DskipTests

    [[xml-replace]]
        REPLACE_1 = 'spark.version', ${spark_jar_version}, pom.xml

[hive_warehouse_connector]
build_tool = sbt

    [[artifacts]]
        artifact_1 = target/scala-2.11/hive-warehouse-connector-assembly-${hive_warehouse_connector_jar_version}.jar
        artifact_2 = target/pyspark_hwc-${hive_warehouse_connector_jar_version}.zip

    [[install_cmd]]
        cmd_1 = ./build/sbt compile -DskipTests -Dversion=${hive_warehouse_connector_jar_version} -Dspark.version=${spark_jar_version} -Dhadoop.version=${hadoop_jar_version} -Dhive.version=${hive_jar_version} -Dtez.version=${tez_jar_version} -Drepourl=${NEXUS_PROXY_URL} -Dgbnurl=${GBN_MVN_REPO}
        cmd_2 = ./build/sbt  assembly -DskipTests -Dversion=${hive_warehouse_connector_jar_version} -Dspark.version=${spark_jar_version} -Dhadoop.version=${hadoop_jar_version} -Dhive.version=${hive_jar_version} -Dtez.version=${tez_jar_version} -Drepourl=${NEXUS_PROXY_URL} -Dgbnurl=${GBN_MVN_REPO}
        cmd_3 = ./build/sbt  publishM2 -DskipTests -Dversion=${hive_warehouse_connector_jar_version} -Dspark.version=${spark_jar_version} -Dhadoop.version=${hadoop_jar_version} -Dhive.version=${hive_jar_version} -Dtez.version=${tez_jar_version} -Drepourl=${NEXUS_PROXY_URL} -Dgbnurl=${GBN_MVN_REPO}

    [[text-replace]]
        REPLACE_1 = 'version :=.*', 'version := "${hive_warehouse_connector_jar_version}"', build.sbt, regex_replace
        REPLACE_2 = 'https://dl.bintray.com/typesafe/ivy-releases', 'https://dl.bintray.com/typesafe/ivy-releases', build/sbt-launch-lib.bash, regex_replace

[superset]
build_tool = npm
node_version = 10.16.0
N_PREFIX = ${HOME}/tools/n
PATH=${N_PREFIX}/n/versions/node/${node_version}/bin:${PATH}

    [[artifacts]]
        artifact_1 = superset-${superset_apache_version}-py3-none-linux_x86_64.wgn

    [[download_modules]]
        cmd_1 = ${cdh_S3_DEV_LOC}/tars/pydruid/pydruid-${pydruid_apache_version}-py2.py3-none-any.whl, ${SOURCE_ROOT}/superset/pydruid-${pydruid_apache_version}-py2.py3-none-any.whl

    [[install_cmd]]
        cmd_1 = npm install n , superset/assets
        cmd_2 = bash -c "N_PREFIX=${N_PREFIX} node_modules/n/bin/n -q ${node_version}" , superset/assets
        cmd_3 = ${N_PREFIX}/n/versions/node/${node_version}/bin/npm ci , superset/assets
        cmd_4 = ${N_PREFIX}/n/versions/node/${node_version}/bin/npm run build , superset/assets
        cmd_5 = rm -rf node_modules , superset/assets
        cmd_6 = ${BASE_DIR}/buildvenv/bin/python setup.py bdist_wheel
        cmd_7 = ${BASE_DIR}/buildvenv/bin/pip download -d wheels -r requirements.txt
        cmd_8 = bash -c "PKGNAME=dist/apache_superset-${superset_apache_version}-py3-none-any.whl ; cp -f $PKGNAME wheels"
        cmd_9 = tar -zcvf superset-${superset_apache_version}-py3-none-linux_x86_64.wgn wheels
        # Uncomment below routine during cldr pydruid integration - ToDo.
        #cmd_10 = rm -rf TMP_DIR
        #cmd_11 = mkdir TMP_DIR
        #cmd_12 = tar -C TMP_DIR -zxvf superset-${superset_apache_version}-py3-none-linux_x86_64.wgn
        #cmd_13 = rm -vf TMP_DIR/wheels/pydruid-${pydruid_apache_version}-py2.py3-none-any.whl
        #cmd_14 = cp -vf pydruid-${pydruid_apache_version}-py2.py3-none-any.whl TMP_DIR/wheels/
        #cmd_15 = tar -zcvf superset-${superset_apache_version}-py3-none-linux_x86_64.wgn wheels/ , TMP_DIR
        #cmd_16 = rm -f superset-${superset_apache_version}-py3-none-linux_x86_64.wgn

    [[test_cmd]]
        cmd_1 = tox -e py3

[qe-examples]
build_tool = maven
setversion_cmd = ${MVN_CMD} ${MVN_SET_VERSION_CMD} -DnewVersion=${qe-examples_jar_version}
deploy_cmd = ${MVN_CMD} package deploy -DskipTests -Dskip

    [[artifacts]]
        artifact_1 = spark2-examples-assembly/target/scala-2.11/jars/spark2-examples-assembly-${spark_jar_version}.jar

    [[install_cmd]]
        cmd_1 = ${MVN_CMD} install -DskipITs -DskipTests -Dmaven.javadoc.skip=true -Dskip=true -Drepoid=${NEXUS_DEPLOY_REPO_ID} -Dreponame=${NEXUS_DEPLOY_REPO_ID} -Drepourl=${NEXUS_REPO_URL}

    [[test_cmd]]
        cmd_1 = ${MVN_CMD} -fae test

    [[xml-replace]]
        #REPLACE_1 = 'flume.version', ${flume_jar_version}, pom.xml
        REPLACE_2 = 'hadoop.version', ${hadoop_jar_version}, pom.xml
        REPLACE_3 = 'hbase.version', ${hbase_jar_version}, pom.xml
        REPLACE_4 = 'protobuf.version', ${protobuf_version}, pom.xml
        REPLACE_5 = 'zookeeper.version', ${zookeeper_jar_version}, pom.xml
        REPLACE_6 = 'kafka.version', ${kafka_jar_version}, pom.xml
        REPLACE_7 = 'calcite.version', ${calcite_jar_version}, pom.xml
        REPLACE_8 = 'spark.version', ${spark_jar_version}, pom.xml

[qm-parcel]
build_tool = maven
setversion_cmd = ${MVN_CMD} ${MVN_SET_VERSION_CMD} -DnewVersion=${qm-parcel_jar_version}
deploy_cmd = ${MVN_CMD} package deploy -DskipTests -Dskip

    [[artifacts]]
        artifact_1 = dist/target/parcel-${computex_cpx_jar_version}.tar.gz

    [[install_cmd]]
        cmd_1 = ${MVN_CMD} install -DskipITs -DskipTests

    [[test_cmd]]
        cmd_1 = ${MVN_CMD} -fae test

    [[xml-replace]]
        REPLACE_1 = 'computex.version', ${computex_cpx_jar_version}, pom.xml

# TODO (sriharsha) figure out spark version from pom.xml , BUG-58027 cached apache archive tar to s3.
# https://github.com/hortonworks/build-support/commit/6a258920f96f89aa7bec590734db69cf878c42a9
[zeppelin]
build_tool = maven
spark_pkg_version = 2.4.0
apache_spark_version = 2.4.0
hadoop_version = 3
COMMON_BUILD_OPTS = ${MVN_CMD} -Divy.home=${HOME}/.ivy2 -Dsbt.ivy.home=${HOME}/.ivy2 -Duser.home=${HOME} -Drepo.maven.org=${NEXUS_REPO_URL} -DaltDeploymentRepository=${NEXUS_DEPLOY_REPO_ID}::default::${NEXUS_REPO_URL} -Dreactor.repo='file://${HOME}/.m2/repository' -pl '!spark/spark1-shims' -Pspark-2.4 -Pscala-2.11 -Dspark.version=${apache_spark_version} -Dhadoop.version=${hadoop_jar_version} -Pyarn -Pbuild-distr -Psparkr -Drat.skip=True -Dspark.src.download.url=http://public-repo-1.hortonworks.com/ARTIFACTS/dist/spark/spark-${spark_pkg_version}/spark-${spark_pkg_version}.tgz -Dspark.bin.download.url=http://public-repo-1.hortonworks.com/ARTIFACTS/dist/spark/spark-${spark_pkg_version}/spark-${spark_pkg_version}-bin-without-hadoop.tgz -Pjdbc-hive -Pjdbc-phoenix -Pjdbc-hadoop3
setversion_cmd = ${MVN_CMD} ${MVN_SET_VERSION_CMD} -DnewVersion=${zeppelin_jar_version}
deploy_cmd = ${COMMON_BUILD_OPTS} deploy -DskipTests
coverage_tool = cobertura

    [[artifacts]]
      artifact_1 = zeppelin-distribution/target/zeppelin-${zeppelin_jar_version}.tar.gz

    [[install_cmd]]
      cmd_1 = ${COMMON_BUILD_OPTS} install -DskipTests

    [[fortify_cmd]]
        cmd_1 = ${COMMON_BUILD_OPTS} install -DskipTests
        cmd_2 = ${COMMON_BUILD_OPTS} -DskipTests -DskipITs ${FORTIFY_ARGS} -Dfortify.sca.buildId=zeppelin-${zeppelin_jar_version} ${FORTIFY_CLEAN_CMD}
        cmd_3 = ${COMMON_BUILD_OPTS} -DskipTests -DskipITs ${FORTIFY_ARGS} -Dfortify.sca.buildId=zeppelin-${zeppelin_jar_version} ${FORTIFY_TRANSLATE_CMD}
        cmd_4 = sourceanalyzer -b zeppelin-${zeppelin_jar_version} ${FORTIFY_SCAN_MEMORY} -scan -f zeppelin-${zeppelin_jar_version}.fpr

    [[xml-replace]]
      REPLACE_1 = 'hadoop.version', ${hadoop_jar_version}, pom.xml
      REPLACE_2 = 'hbase.hadoop.version', ${hadoop_jar_version}, pom.xml
      REPLACE_3 = 'hadoop-common.version', ${hadoop_jar_version}, pom.xml
      REPLACE_4 = 'hbase.version', ${hbase_jar_version}, pom.xml
      REPLACE_5 = 'hbase.hbase.version', ${hbase_jar_version}, pom.xml
      REPLACE_6 = 'spark.version', ${apache_spark_version}, pom.xml
      REPLACE_7 = 'protobuf.version', ${protobuf_version}, pom.xml
      REPLACE_8 = 'zookeeper.version' , ${zookeeper_jar_version} , pom.xml
      REPLACE_10 = 'hive.version', ${hive_jar_version}, jdbc/pom.xml
      REPLACE_11 = 'phoenix.version', ${phoenix_jar_version}, pom.xml
      REPLACE_12 = 'spark-hive.version', ${spark_hive2_jar_version}, jdbc/pom.xml
      REPLACE_13 = 'hive2.version', ${hive_jar_version}, jdbc/pom.xml
      REPLACE_14 = 'gcs.version' , ${gcs_jar_version} , zeppelin-zengine/pom.xml

    [[test_cmd]]
      cmd_1 = ${MVN_CMD} -Drat.skip=true -DfailIfNoTests=false -fae test

    [[test_coverage_cmd]]
      cmd_1 = ${MVN_CMD} -Drat.skip=true -DfailIfNoTests=false -fae cobertura:cobertura

[tez]
build_tool = maven
COMMON_BUILD_OPTS = "${MVN_CMD} clean ${MAVEN_TEST_OPTS} -Dtar -Pgpg -Paws -Pazure -Pgcs -Phadoop28 -Psources -Dhadoop.version=${hadoop_jar_version} -Drepoid=${NEXUS_DEPLOY_REPO_ID} -Dreponame=${NEXUS_DEPLOY_REPO_ID} -Dinternal.maven.repository=${NEXUS_REPO_URL} -DdistMgmtStagingId=${NEXUS_DEPLOY_REPO_ID} -DdistMgmtStagingUrl=${NEXUS_REPO_URL}"
setversion_cmd = ${MVN_CMD} ${MVN_SET_VERSION_CMD} -DnewVersion=${tez_jar_version}
deploy_cmd = ${COMMON_BUILD_OPTS} deploy
package_count = 2
coverage_tool = cobertura

    [[artifacts]]
        artifact_1 = tez-dist/target/tez-${tez_jar_version}-minimal.tar.gz
        artifact_2 = tez-dist/target/tez-${tez_jar_version}.tar.gz

    [[install_cmd]]
        cmd_1 = ${COMMON_BUILD_OPTS} install -DskipTests

    [[fortify_cmd]]
        cmd_1 = "${COMMON_BUILD_OPTS} -DskipITs -DskipTests install"
        cmd_2 = ${COMMON_BUILD_OPTS} -DskipTests -DskipITs -Dmaven.javadoc.skip=true ${FORTIFY_ARGS} -Dfortify.sca.buildId=tez-${tez_jar_version} ${FORTIFY_CLEAN_CMD}
        cmd_3 = ${COMMON_BUILD_OPTS} -DskipTests -DskipITs -Dmaven.javadoc.skip=true ${FORTIFY_ARGS} -Dfortify.sca.buildId=tez-${tez_jar_version} ${FORTIFY_TRANSLATE_CMD}
        cmd_4 = sourceanalyzer -b tez-${tez_jar_version} ${FORTIFY_SCAN_MEMORY} -scan -f tez-${tez_jar_version}.fpr

    [[test_cmd]]
        cmd_1 = npm set strict-ssl false ; ${MVN_CMD} test

    [[test_coverage_cmd]]
        cmd_1 = npm set strict-ssl false ; ${MVN_CMD} cobertura:cobertura

    [[xml-replace]]
        REPLACE_1 = 'hadoop.version', ${hadoop_jar_version}, pom.xml
        REPLACE_2 = 'zookeeper.version', ${zookeeper_jar_version}, pom.xml
        REPLACE_3 = 'gcs.version' , ${gcs_jar_version} , pom.xml


[zookeeper]
build_tool = maven
COMMON_BUILD_OPTS ="${MVN_CMD} -Drepoid=${NEXUS_DEPLOY_REPO_ID} -Dreponame=${NEXUS_DEPLOY_REPO_ID} -Drepourl=${NEXUS_REPO_URL} -Drepo.maven.org=${NEXUS_PROXY_URL}"
deploy_cmd = "${COMMON_BUILD_OPTS} deploy -DskipTests"
package_count = 3
coverage_tool = cobertura

    [[artifacts]]
        artifact_1 = zookeeper-assembly/target/apache-zookeeper-${zookeeper_jar_version}.tar.gz
        artifact_2 = zookeeper-assembly/target/apache-zookeeper-${zookeeper_jar_version}-bin.tar.gz

    [[setversion_cmd]]
        cmd_1 = ${MVN_CMD} ${MVN_SET_VERSION_CMD} -DnewVersion=${zookeeper_jar_version} -P full-build

    [[install_cmd]]
        cmd_1 = ${COMMON_BUILD_OPTS} clean install -DskipTests -P full-build

    [[fortify_cmd]]
        cmd_1 = ${COMMON_BUILD_OPTS} clean install -DskipTests
        cmd_2 = ${COMMON_BUILD_OPTS} -DskipTests ${FORTIFY_ARGS} -Dfortify.sca.buildId=zookeeper-${zookeeper_jar_version} ${FORTIFY_CLEAN_CMD}
        cmd_3 = ${COMMON_BUILD_OPTS} -DskipTests ${FORTIFY_ARGS} -Dfortify.sca.buildId=zookeeper-${zookeeper_jar_version} ${FORTIFY_TRANSLATE_CMD}
        cmd_4 = sourceanalyzer -b zookeeper-${zookeeper_jar_version} ${FORTIFY_SCAN_MEMORY} -scan -f zookeeper-${zookeeper_jar_version}.fpr

    [[test_cmd]]
        cmd_1 = ${COMMON_BUILD_OPTS} -Dsurefire.rerunFailingTestsCount=2 -DfailIfNoTests=false -fae test

    [[test_coverage_cmd]]
        cmd_1 = ${COMMON_BUILD_OPTS} clean cobertura:cobertura

    [[xml-replace]]
        REPLACE_1 = 'commons-cli.version', ${cdpd_commons-cli_version}, pom.xml
        REPLACE_2 = 'commons-collections.version', ${cdpd_commons-collections_version}, pom.xml
        REPLACE_3 = 'jackson.version', ${cdpd_jackson2_version}, pom.xml
        REPLACE_4 = 'jetty.version', ${cdpd_jetty9_version}, pom.xml
        REPLACE_5 = 'jline.version', ${cdpd_jline_version}, pom.xml
        REPLACE_6 = 'kerby.version', ${cdpd_kerby_version}, pom.xml
        REPLACE_7 = 'junit.version', ${cdpd_junit_version}, pom.xml
        REPLACE_8 = 'log4j.version', ${cdpd_log4j_version}, pom.xml
        REPLACE_9 = 'mockito.version', ${cdpd_mockito_version}, pom.xml
        REPLACE_10 = 'netty.version', ${cdpd_netty4_version}, pom.xml
        REPLACE_11 = 'slf4j.version', ${cdpd_slf4j_version}, pom.xml
